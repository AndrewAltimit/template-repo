name: Scheduled Scraping

on:
  schedule:
    # Run monthly on the 1st at 4 AM UTC
    - cron: '0 4 1 * *'
  workflow_dispatch:
    inputs:
      states:
        description: 'States to scrape (comma-separated, or "all")'
        required: false
        default: 'all'
        type: string

jobs:
  scrape-requirements:
    name: Scrape State Requirements
    runs-on: ubuntu-latest
    strategy:
      matrix:
        state: [oregon, massachusetts, rhode_island, washington, delaware, connecticut, vermont, colorado]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip packages
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-scraper-${{ hashFiles('requirements-cgt.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-scraper-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements-cgt.txt
          pip install -e .

      - name: Run scraper for ${{ matrix.state }}
        run: |
          export PYTHONPATH=${{ github.workspace }}/src:$PYTHONPATH

          # Check if we should scrape this state
          if [ "${{ github.event.inputs.states }}" != "all" ] && [ -n "${{ github.event.inputs.states }}" ]; then
            if ! echo "${{ github.event.inputs.states }}" | grep -q "${{ matrix.state }}"; then
              echo "Skipping ${{ matrix.state }} - not in requested states"
              exit 0
            fi
          fi

          echo "Scraping requirements for ${{ matrix.state }}..."
          python -m scrapers.web_scraper ${{ matrix.state }} --output-dir ./scraped_docs/${{ matrix.state }}

      - name: Check for changes
        id: check_changes
        run: |
          if [ -d "./scraped_docs/${{ matrix.state }}" ]; then
            if [ -n "$(ls -A ./scraped_docs/${{ matrix.state }})" ]; then
              echo "DOCS_FOUND=true" >> $GITHUB_ENV
              echo "Found documents for ${{ matrix.state }}"
            else
              echo "DOCS_FOUND=false" >> $GITHUB_ENV
              echo "No documents found for ${{ matrix.state }}"
            fi
          else
            echo "DOCS_FOUND=false" >> $GITHUB_ENV
          fi

      - name: Upload scraped documents
        if: env.DOCS_FOUND == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: scraped-docs-${{ matrix.state }}
          path: ./scraped_docs/${{ matrix.state }}/
          retention-days: 30

  create-summary:
    name: Create Scraping Summary
    runs-on: ubuntu-latest
    needs: scrape-requirements
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: ./all-scraped-docs/

      - name: Create summary report
        run: |
          echo "# CGT Requirements Scraping Summary" > SCRAPING_SUMMARY.md
          echo "" >> SCRAPING_SUMMARY.md
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> SCRAPING_SUMMARY.md
          echo "" >> SCRAPING_SUMMARY.md
          echo "## States Processed" >> SCRAPING_SUMMARY.md
          echo "" >> SCRAPING_SUMMARY.md

          for state_dir in ./all-scraped-docs/scraped-docs-*/; do
            if [ -d "$state_dir" ]; then
              state_name=$(basename "$state_dir" | sed 's/scraped-docs-//')
              doc_count=$(find "$state_dir" -type f | wc -l)
              echo "- **${state_name}**: ${doc_count} documents" >> SCRAPING_SUMMARY.md
            fi
          done

          echo "" >> SCRAPING_SUMMARY.md
          echo "## Next Steps" >> SCRAPING_SUMMARY.md
          echo "" >> SCRAPING_SUMMARY.md
          echo "1. Review the scraped documents in the artifacts" >> SCRAPING_SUMMARY.md
          echo "2. Update validators if requirements have changed" >> SCRAPING_SUMMARY.md
          echo "3. Run validation tests to ensure compatibility" >> SCRAPING_SUMMARY.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: scraping-summary
          path: SCRAPING_SUMMARY.md

      - name: Create issue if documents found
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('SCRAPING_SUMMARY.md', 'utf8');

            // Check if any documents were found
            if (summary.includes('documents')) {
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: `Monthly CGT Requirements Update - ${new Date().toISOString().split('T')[0]}`,
                body: summary + '\n\n---\n\n**Automated issue created by scheduled scraping workflow**',
                labels: ['requirements-update', 'automated']
              });
              console.log(`Created issue #${issue.data.number}`);
            }
