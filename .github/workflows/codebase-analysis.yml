---
# Daily Codebase Analysis Pipeline
#
# Automatically analyzes the codebase with multiple AI agents and
# creates GitHub issues for discovered improvements. Issues require
# admin approval via [Approved] comment before PRs are created.
#
# STATUS: MANUAL-ONLY - Schedule disabled until ready for production
#
# To enable:
# 1. Uncomment the schedule section
# 2. Configure codebase-analysis-config.yml
# 3. Ensure AGENT_TOKEN and GH_PROJECTS_TOKEN secrets are set

name: Codebase Analysis Pipeline

on:
  # DISABLED: Uncomment to enable scheduled runs
  # schedule:
  #   # Daily at 3 AM UTC (off-peak hours)
  #   - cron: '0 3 * * *'

  # Manual trigger for testing
  workflow_dispatch:
    inputs:
      agents:
        description: 'Agents to use (comma-separated)'
        required: false
        type: string
        default: 'claude,gemini'
      max-issues:
        description: 'Maximum issues to create'
        required: false
        type: number
        default: 3
      min-priority:
        description: 'Minimum priority (P0-P3)'
        required: false
        type: choice
        options:
          - P0
          - P1
          - P2
          - P3
        default: 'P2'
      dry-run:
        description: 'Dry run (analyze but do not create issues)'
        required: false
        type: boolean
        default: true
      include-paths:
        description: 'Paths to analyze (comma-separated globs)'
        required: false
        type: string
        default: 'packages/**/*.py,tools/**/*.py'

permissions:
  contents: read
  issues: write

concurrency:
  group: codebase-analysis
  cancel-in-progress: false

jobs:
  analyze:
    name: Analyze Codebase
    runs-on: self-hosted
    timeout-minutes: 45

    outputs:
      findings-count: ${{ steps.analyze.outputs.findings_count }}
      issues-created: ${{ steps.create.outputs.issues_created }}
      summary-json: ${{ steps.create.outputs.summary_json }}

    steps:
      # Fix Docker-created directory permissions BEFORE checkout
      # Previous runs may have created files as root which blocks checkout cleanup
      # NOTE: Uses Docker (not sudo) to fix permissions - avoids password prompt hangs
      - name: Fix Docker directory permissions (pre-checkout)
        run: |
          echo "::group::Pre-Checkout: Fix Docker Directory Permissions"
          WORKSPACE="${GITHUB_WORKSPACE:-$(pwd)}"

          # List of directories that Docker may have created with root ownership
          # Using direct paths instead of 'find' to avoid hangs on permission-denied dirs
          DIRS_TO_FIX=(
            "outputs"
            "evaluation_results"
            ".agent-venv"
            "__pycache__"
            ".pytest_cache"
          )

          # Fix permissions using Docker (runs as root, no sudo password needed)
          for dir in "${DIRS_TO_FIX[@]}"; do
            if [ -d "$WORKSPACE/$dir" ]; then
              echo "Fixing ownership via Docker: $WORKSPACE/$dir"
              # Use busybox to chown the directory to the current user
              # timeout prevents hanging if Docker has issues
              timeout 30 docker run --rm \
                -v "$WORKSPACE/$dir:/target" \
                busybox \
                chown -R "$(id -u):$(id -g)" /target 2>/dev/null || echo "  Warning: Could not fix $dir (may already have correct ownership)"
            fi
          done

          echo "Permissions check complete"
          echo "::endgroup::"

      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0  # Full history for analysis

      - name: Setup Python environment
        run: |
          echo "::group::Setup Python Environment"
          python3 --version

          # Try to create virtual environment, fall back to --user install if venv unavailable
          if python3 -m venv .venv 2>/dev/null; then
            echo "[INFO] Virtual environment created successfully"
            source .venv/bin/activate
            PYTHON_CMD="$PWD/.venv/bin/python3"
            PIP_USER_FLAG=""
          else
            echo "[WARNING] python3-venv not available, using --user install"
            PYTHON_CMD="python3"
            PIP_USER_FLAG="--user"
            export PATH="$HOME/.local/bin:$PATH"
          fi

          $PYTHON_CMD -m pip install $PIP_USER_FLAG -e ./packages/github_agents
          echo "PYTHON_CMD=$PYTHON_CMD" >> $GITHUB_ENV
          echo "PATH=$HOME/.local/bin:$PATH" >> $GITHUB_ENV
          echo "::endgroup::"

      - name: Load configuration
        id: config
        run: |
          echo "::group::Load Configuration"

          # Use inputs or defaults
          AGENTS="${{ inputs.agents || 'claude,gemini' }}"
          MAX_ISSUES="${{ inputs.max-issues || '3' }}"
          MIN_PRIORITY="${{ inputs.min-priority || 'P2' }}"
          INCLUDE_PATHS="${{ inputs.include-paths || 'packages/**/*.py,tools/**/*.py' }}"
          DRY_RUN="${{ inputs.dry-run }}"

          echo "agents=$AGENTS" >> $GITHUB_OUTPUT
          echo "max_issues=$MAX_ISSUES" >> $GITHUB_OUTPUT
          echo "min_priority=$MIN_PRIORITY" >> $GITHUB_OUTPUT
          echo "include_paths=$INCLUDE_PATHS" >> $GITHUB_OUTPUT
          echo "dry_run=$DRY_RUN" >> $GITHUB_OUTPUT

          echo "Configuration:"
          echo "  Agents: $AGENTS"
          echo "  Max Issues: $MAX_ISSUES"
          echo "  Min Priority: $MIN_PRIORITY"
          echo "  Include Paths: $INCLUDE_PATHS"
          echo "  Dry Run: $DRY_RUN"

          echo "::endgroup::"

      - name: Run codebase analysis
        id: analyze
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          AGENTS: ${{ steps.config.outputs.agents }}
          INCLUDE_PATHS: ${{ steps.config.outputs.include_paths }}
        run: |
          echo "::group::Codebase Analysis"

          # Use configured Python command to access installed packages
          ANALYSIS_OUTPUT=$($PYTHON_CMD << 'PYTHON_SCRIPT'
          import asyncio
          import json
          import os
          import sys
          from pathlib import Path
          from dataclasses import asdict

          async def run_analysis():
              findings = []
              agents = os.environ.get('AGENTS', 'claude').split(',')
              include_paths = os.environ.get('INCLUDE_PATHS', '**/*.py').split(',')

              print(f"Running analysis with agents: {agents}", file=sys.stderr)
              print(f"Include paths: {include_paths}", file=sys.stderr)

              try:
                  from github_agents.analyzers import AgentAnalyzer, FindingCategory
                  from github_agents.agents import (
                      get_best_available_agent,
                      ClaudeAgent,
                      GeminiAgent,
                      CodexAgent,
                      OpenCodeAgent,
                  )

                  # Map agent names to classes and their focus areas
                  agent_configs = {
                      "claude": {
                          "class": ClaudeAgent,
                          "prompt": "Analyze this codebase for architectural issues, design patterns, and refactoring opportunities.",
                          "categories": [FindingCategory.QUALITY, FindingCategory.ARCHITECTURE],
                      },
                      "gemini": {
                          "class": GeminiAgent,
                          "prompt": "Analyze this codebase for security vulnerabilities, code smells, and best practice violations.",
                          "categories": [FindingCategory.SECURITY, FindingCategory.QUALITY],
                      },
                      "codex": {
                          "class": CodexAgent,
                          "prompt": "Analyze this codebase for performance bottlenecks and optimization opportunities.",
                          "categories": [FindingCategory.PERFORMANCE],
                      },
                      "opencode": {
                          "class": OpenCodeAgent,
                          "prompt": "Analyze this codebase for technical debt, outdated dependencies, and maintenance issues.",
                          "categories": [FindingCategory.TECH_DEBT, FindingCategory.DOCUMENTATION],
                      },
                  }

                  repo_path = Path.cwd()

                  # Run analysis with each requested agent
                  for agent_name in agents:
                      agent_name = agent_name.strip().lower()
                      if agent_name not in agent_configs:
                          print(f"Unknown agent: {agent_name}, skipping", file=sys.stderr)
                          continue

                      config = agent_configs[agent_name]
                      print(f"Analyzing with {agent_name}...", file=sys.stderr)

                      try:
                          # Create agent instance
                          agent_instance = config["class"]()

                          # Check if agent is available
                          if not agent_instance.is_available():
                              print(f"Agent {agent_name} is not available, skipping", file=sys.stderr)
                              continue

                          # Create analyzer with agent
                          analyzer = AgentAnalyzer(
                              agent_name=agent_name,
                              agent_instance=agent_instance,
                              analysis_prompt=config["prompt"],
                              categories=config["categories"],
                              include_paths=include_paths,
                          )

                          # Run analysis
                          agent_findings = await analyzer.analyze(repo_path)

                          # Convert to serializable format
                          for finding in agent_findings:
                              findings.append({
                                  "title": finding.title,
                                  "category": finding.category.value,
                                  "priority": finding.priority.value,
                                  "summary": finding.summary,
                                  "details": finding.details,
                                  "files": [asdict(af) for af in finding.affected_files],
                                  "suggested_fix": finding.suggested_fix,
                                  "evidence": finding.evidence,
                                  "agent": finding.discovered_by,
                              })

                          print(f"Agent {agent_name} found {len(agent_findings)} issues", file=sys.stderr)

                      except Exception as e:
                          print(f"Error running agent {agent_name}: {e}", file=sys.stderr)
                          continue

                  print(f"Analysis complete. Found {len(findings)} total findings.", file=sys.stderr)

              except ImportError as e:
                  print(f"Import error: {e}", file=sys.stderr)
                  print("Ensure github_agents package is installed", file=sys.stderr)

              return {"findings": findings, "count": len(findings)}

          result = asyncio.run(run_analysis())
          print(json.dumps(result))
          PYTHON_SCRIPT
          )

          echo "Analysis output: $ANALYSIS_OUTPUT"

          FINDINGS_COUNT=$(echo "$ANALYSIS_OUTPUT" | python3 -c "import sys,json; print(json.load(sys.stdin).get('count', 0))")
          echo "findings_count=$FINDINGS_COUNT" >> $GITHUB_OUTPUT

          # Pass full findings JSON to next step (escape for multiline)
          echo "findings_json<<EOF" >> $GITHUB_OUTPUT
          echo "$ANALYSIS_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "::endgroup::"

      - name: Create issues from findings
        id: create
        if: steps.analyze.outputs.findings_count != '0'
        env:
          GITHUB_TOKEN: ${{ secrets.AGENT_TOKEN }}
          GITHUB_PROJECTS_TOKEN: ${{ secrets.GH_PROJECTS_TOKEN }}
          DRY_RUN: ${{ steps.config.outputs.dry_run }}
          MAX_ISSUES: ${{ steps.config.outputs.max_issues }}
          MIN_PRIORITY: ${{ steps.config.outputs.min_priority }}
          ANALYSIS_OUTPUT: ${{ steps.analyze.outputs.findings_json }}
        run: |
          echo "::group::Create Issues"

          CREATE_OUTPUT=$(python3 << 'PYTHON_SCRIPT'
          import asyncio
          import json
          import os
          import sys

          async def create_issues():
              dry_run = os.environ.get('DRY_RUN', 'true').lower() == 'true'
              max_issues = int(os.environ.get('MAX_ISSUES', '3'))
              min_priority = os.environ.get('MIN_PRIORITY', 'P2')
              repo = os.environ.get('GITHUB_REPOSITORY', '')

              # Priority ordering for filtering
              priority_order = {'P0': 0, 'P1': 1, 'P2': 2, 'P3': 3}
              min_priority_value = priority_order.get(min_priority, 2)

              # Parse findings from analysis step
              findings_json = os.environ.get('ANALYSIS_OUTPUT', '{}')
              try:
                  analysis_result = json.loads(findings_json) if findings_json else {}
                  findings = analysis_result.get('findings', [])
              except json.JSONDecodeError:
                  print("Failed to parse findings JSON", file=sys.stderr)
                  findings = []

              if not findings:
                  print("No findings to process", file=sys.stderr)
                  return {"issues_created": 0, "skipped": 0, "errors": 0}

              print(f"Processing {len(findings)} findings (max: {max_issues}, min priority: {min_priority})", file=sys.stderr)

              # Filter by priority
              filtered_findings = []
              for f in findings:
                  f_priority = priority_order.get(f.get('priority', 'P3'), 3)
                  if f_priority <= min_priority_value:
                      filtered_findings.append(f)
                  else:
                      print(f"Skipping {f.get('title', 'untitled')} - priority {f.get('priority')} below threshold", file=sys.stderr)

              # Limit to max_issues
              filtered_findings = filtered_findings[:max_issues]

              if dry_run:
                  print(f"DRY RUN: Would create {len(filtered_findings)} issues:", file=sys.stderr)
                  for f in filtered_findings:
                      print(f"  - [{f.get('priority')}] {f.get('title')}", file=sys.stderr)
                  return {"issues_created": 0, "would_create": len(filtered_findings), "dry_run": True}

              # Import and use IssueCreator with BoardManager
              try:
                  from github_agents.creators import IssueCreator
                  from github_agents.analyzers import AnalysisFinding, AffectedFile, FindingCategory, FindingPriority
                  from github_agents.board.manager import BoardManager

                  # Convert dicts back to AnalysisFinding objects
                  findings_list = []
                  for f in filtered_findings:
                      files_data = f.get('files', [])
                      affected_files = [
                          AffectedFile(**file_dict) if isinstance(file_dict, dict)
                          else AffectedFile(path=str(file_dict))
                          for file_dict in files_data
                      ]
                      finding = AnalysisFinding(
                          title=f.get('title', 'Untitled Finding'),
                          category=FindingCategory(f.get('category', 'quality')),
                          priority=FindingPriority(f.get('priority', 'P2')),
                          summary=f.get('summary', ''),
                          details=f.get('details', ''),
                          affected_files=affected_files,
                          suggested_fix=f.get('suggested_fix', ''),
                          evidence=f.get('evidence', ''),
                          discovered_by=f.get('agent', 'unknown'),
                      )
                      findings_list.append(finding)

                  # Initialize BoardManager for project board integration
                  board_manager = None
                  try:
                      board_manager = BoardManager()
                      await board_manager.initialize()
                      print("BoardManager initialized - issues will be added to project board", file=sys.stderr)
                  except Exception as e:
                      print(f"BoardManager init failed (continuing without board): {e}", file=sys.stderr)
                      board_manager = None

                  # Create issues using the proper API
                  creator = IssueCreator(
                      repo=repo,
                      board_manager=board_manager,
                      lookback_days=30,
                      max_issues_per_run=max_issues,
                      min_priority=FindingPriority(min_priority),
                  )

                  results = await creator.create_issues(findings_list)

                  # Clean up BoardManager session
                  if board_manager:
                      await board_manager.close()

                  # Process results
                  issues_created = 0
                  skipped = 0
                  for result in results:
                      if result.created:
                          issues_created += 1
                          print(f"Created issue #{result.issue_number}: {result.finding.title}", file=sys.stderr)
                      elif result.skipped_reason:
                          skipped += 1
                          print(f"Skipped: {result.skipped_reason}", file=sys.stderr)

                  return {"issues_created": issues_created, "skipped": skipped, "errors": 0}

              except ImportError as e:
                  print(f"Import error: {e}", file=sys.stderr)
                  return {"issues_created": 0, "error": str(e)}

          result = asyncio.run(create_issues())
          print(json.dumps(result))
          PYTHON_SCRIPT
          )

          echo "Create output: $CREATE_OUTPUT"

          ISSUES_CREATED=$(echo "$CREATE_OUTPUT" | python3 -c "import sys,json; print(json.load(sys.stdin).get('issues_created', 0))")
          echo "issues_created=$ISSUES_CREATED" >> $GITHUB_OUTPUT

          echo "summary_json=$CREATE_OUTPUT" >> $GITHUB_OUTPUT
          echo "::endgroup::"

      - name: Generate summary
        if: always()
        env:
          FINDINGS_COUNT: ${{ steps.analyze.outputs.findings_count }}
          ISSUES_CREATED: ${{ steps.create.outputs.issues_created }}
          DRY_RUN: ${{ steps.config.outputs.dry_run }}
        run: |
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## Codebase Analysis Pipeline Results

          | Metric | Value |
          |--------|-------|
          | Findings Discovered | ${FINDINGS_COUNT:-0} |
          | Issues Created | ${ISSUES_CREATED:-0} |
          | Dry Run | ${DRY_RUN:-true} |

          ### Configuration
          - **Agents**: ${{ steps.config.outputs.agents }}
          - **Min Priority**: ${{ steps.config.outputs.min_priority }}
          - **Max Issues**: ${{ steps.config.outputs.max_issues }}

          ### Next Steps
          1. Review created issues in the [Issues tab](../../issues)
          2. Approve issues with \`[Approved]\` comment to trigger PR creation
          3. Issue Monitor will create PRs for approved issues

          ---
          *Run: $(date -u '+%Y-%m-%d %H:%M:%S UTC')*
          EOF

      # Fix Docker-created directory permissions after job completion
      # Ensures subsequent runs won't have permission issues during checkout
      # NOTE: Uses Docker (not sudo) to fix permissions - avoids password prompt hangs
      - name: Fix Docker directory permissions (post-cleanup)
        if: always()
        run: |
          echo "::group::Post-Cleanup: Fix Docker Directory Permissions"
          WORKSPACE="${GITHUB_WORKSPACE:-$(pwd)}"

          # List of directories that Docker may have created with root ownership
          DIRS_TO_FIX=(
            "outputs"
            "evaluation_results"
            ".agent-venv"
            "__pycache__"
            ".pytest_cache"
          )

          # Fix permissions using Docker (runs as root, no sudo password needed)
          for dir in "${DIRS_TO_FIX[@]}"; do
            if [ -d "$WORKSPACE/$dir" ]; then
              echo "Fixing ownership via Docker: $WORKSPACE/$dir"
              timeout 30 docker run --rm \
                -v "$WORKSPACE/$dir:/target" \
                busybox \
                chown -R "$(id -u):$(id -g)" /target 2>/dev/null || echo "  Warning: Could not fix $dir"
            fi
          done

          echo "Post-cleanup complete"
          echo "::endgroup::"

  notify:
    name: Notify on Issues Created
    needs: analyze
    runs-on: self-hosted
    if: needs.analyze.outputs.issues-created != '0'

    steps:
      - name: Send notification
        run: |
          echo "Created ${{ needs.analyze.outputs.issues-created }} issues from analysis"

          # CUSTOMIZE: Add notification logic (Slack, email, etc.)
          # curl -X POST ${{ secrets.SLACK_WEBHOOK }} ...
