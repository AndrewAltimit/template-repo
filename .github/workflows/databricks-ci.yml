name: Databricks Environment CI

on:
  push:
    branches: [databricks-env-setup]
    paths:
      - 'databricks/**'
      - '.github/workflows/databricks-ci.yml'
  pull_request:
    branches: [databricks-env-setup]
    paths:
      - 'databricks/**'
      - '.github/workflows/databricks-ci.yml'

jobs:
  # Build Docker images first - these will be used for testing
  build-docker-images:
    runs-on: [self-hosted, linux]
    strategy:
      matrix:
        dbr-version: [15, 16]

    steps:
      - uses: actions/checkout@v4

      - name: Build DBR${{ matrix.dbr-version }} Docker image
        run: |
          cd databricks
          docker build -f reference/dockerfiles/dbr${{ matrix.dbr-version }}.Dockerfile \
            -t dbr-env:${{ matrix.dbr-version }} .

      - name: Test container has all tools
        run: |
          # Quick smoke test to ensure image built correctly
          docker run --rm --user 1000:1000 dbr-env:${{ matrix.dbr-version }} sh -c "
            echo '--- Testing Python ---' && python --version && \
            echo '--- Testing Java ---' && java -version && \
            echo '--- Testing Databricks CLI ---' && databricks --version && \
            echo '--- Testing Terraform ---' && terraform version && \
            echo '--- Testing Terragrunt ---' && terragrunt --version && \
            echo '--- Testing AWS CLI ---' && aws --version && \
            echo '✓ All tools verified successfully'
          "

  build-wheels:
    runs-on: [self-hosted, linux]
    container:
      image: python:3.11-slim
      options: --user 1000:1000
    strategy:
      matrix:
        package: [core, ml, cloud, all]

    steps:
      - uses: actions/checkout@v4

      - name: Install build tools
        run: |
          pip install --user --upgrade pip
          pip install --user build twine

      - name: Build wheel for ${{ matrix.package }}
        run: |
          export PATH=$PATH:$HOME/.local/bin
          cd databricks/wheels/dbr-env-${{ matrix.package }}
          python -m build --wheel

      - name: Validate wheel
        run: |
          export PATH=$PATH:$HOME/.local/bin
          cd databricks/wheels/dbr-env-${{ matrix.package }}
          python -m twine check dist/*.whl

      - name: Upload wheel
        uses: actions/upload-artifact@v4
        with:
          name: wheel-${{ matrix.package }}
          path: databricks/wheels/dbr-env-${{ matrix.package }}/dist/*.whl
          retention-days: 7

  test-python-packages:
    needs: [build-wheels, build-docker-images]
    runs-on: [self-hosted, linux]
    strategy:
      matrix:
        include:
          - dbr-version: dbr15
            docker-tag: 15
          - dbr-version: dbr16
            docker-tag: 16
    container:
      image: dbr-env:${{ matrix.docker-tag }}
      options: --user 1000:1000

    steps:
      - uses: actions/checkout@v4

      - name: Download wheels
        uses: actions/download-artifact@v4
        with:
          pattern: wheel-*
          path: dist/
          merge-multiple: true

      - name: Install wheels
        run: |
          pip install --upgrade pip
          pip install dist/dbr_env_core-*.whl
          pip install dist/dbr_env_ml-*.whl
          pip install dist/dbr_env_cloud-*.whl
          pip install dist/dbr_env_all-*.whl

      - name: Install DBR dependencies
        run: |
          pip install "dbr-env-all[${{ matrix.dbr-version }}]"

      - name: Test imports
        run: |
          python -c "
          from dbr_env_core import get_dbr_info
          from dbr_env_ml import get_ml_info
          from dbr_env_cloud import get_cloud_info
          from dbr_env_all import get_all_info
          print('✓ All imports successful')

          info = get_all_info('${{ matrix.dbr-version }}')
          print(f'DBR Info: {info}')
          "

      - name: Test mock functionality
        run: |
          python -c "
          from dbr_env_core.mock import get_mock_spark_session, get_mock_databricks_client

          # Test mock Spark
          spark = get_mock_spark_session('TestApp')
          df = spark.createDataFrame([{'test': 'data'}])
          print(f'Mock Spark DataFrame count: {df.count()}')

          # Test mock Databricks
          client = get_mock_databricks_client()
          clusters = client.clusters.list()
          print(f'Mock clusters: {len(clusters)}')

          print('✓ Mock tests passed')
          "

      - name: Run validation (should pass all checks)
        run: |
          # This should now pass all checks since we're in the complete DBR environment
          python -m dbr_env_all.validate --version ${{ matrix.dbr-version }} --json

          # Also run without JSON to see human-readable output
          python -m dbr_env_all.validate --version ${{ matrix.dbr-version }}

  test-scripts:
    runs-on: [self-hosted, linux]
    container:
      image: python:3.11-slim
      options: --user 1000:1000
    steps:
      - uses: actions/checkout@v4

      - name: Test validation script availability
        run: |
          # Note: dbr-validate is provided by the Python package, not a bash script
          # It will be available after installing the dbr-env-all package
          echo "dbr-validate is provided by the Python dbr-env-all package"

      - name: Test pre-installation script
        run: |
          databricks/scripts/dbr-setup-pre --help

      - name: Test post-installation script
        run: |
          databricks/scripts/dbr-setup-post --help

  integration-test:
    needs: [test-python-packages, test-scripts, build-docker-images]
    runs-on: [self-hosted, linux]
    container:
      image: dbr-env:15
      options: --user 1000:1000
    steps:
      - uses: actions/checkout@v4

      - name: Download wheels
        uses: actions/download-artifact@v4
        with:
          pattern: wheel-*
          path: dist/
          merge-multiple: true

      - name: Run integration test
        run: |
          # Install packages
          pip install --upgrade pip
          pip install dist/*.whl
          pip install "dbr-env-all[dbr15]"

          # Test complete flow
          python -c "
          from dbr_env_all import get_all_info
          from dbr_env_core.mock import get_mock_spark_session

          # Get environment info
          info = get_all_info('dbr15')
          assert 'core' in info
          assert 'ml' in info
          assert 'cloud' in info

          # Test mock Spark
          spark = get_mock_spark_session()
          df = spark.createDataFrame([
              {'name': 'test', 'value': 123}
          ])
          assert df.count() == 1

          print('✓ Integration test passed')
          "

      - name: Run final validation
        run: |
          # Final validation in the DBR15 environment
          python -m dbr_env_all.validate --version dbr15 --json
