name: Databricks Environment CI

on:
  push:
    branches: [databricks-env-setup]
    paths:
      - 'databricks/**'
      - '.github/workflows/databricks-ci.yml'
  pull_request:
    branches: [databricks-env-setup]
    paths:
      - 'databricks/**'
      - '.github/workflows/databricks-ci.yml'

jobs:
  # Build Docker images first - these will be used for testing
  build-docker-images:
    runs-on: [self-hosted, linux]
    strategy:
      matrix:
        dbr-version: [15, 16]

    steps:
      - uses: actions/checkout@v4

      - name: Build DBR${{ matrix.dbr-version }} Docker image
        run: |
          cd databricks
          # Export versions from config/versions.json for Docker build
          source ./scripts/export-versions.sh
          # Use the compose script which auto-detects docker-compose vs podman-compose
          ./run-compose.sh build dbr${{ matrix.dbr-version }}

      - name: Test container has all tools
        run: |
          cd databricks
          # Quick smoke test to ensure image built correctly
          ./run-compose.sh run --rm dbr${{ matrix.dbr-version }} sh -c "
            echo '--- Testing Python ---' && python --version && \
            echo '--- Testing Java ---' && java -version && \
            echo '--- Testing Databricks CLI ---' && databricks --version && \
            echo '--- Testing Terraform ---' && terraform version && \
            echo '--- Testing Terragrunt ---' && terragrunt --version && \
            echo '--- Testing AWS CLI ---' && aws --version && \
            echo '✓ All tools verified successfully'
          "

  build-wheels:
    runs-on: [self-hosted, linux]
    container:
      image: python:3.11-slim
      options: --user 1000:1000
    strategy:
      matrix:
        package: [core, ml, cloud, all]

    steps:
      - uses: actions/checkout@v4

      - name: Install build tools
        run: |
          pip install --user --upgrade pip
          pip install --user build twine

      - name: Build wheel for ${{ matrix.package }}
        run: |
          export PATH=$PATH:$HOME/.local/bin
          cd databricks/wheels/dbr-env-${{ matrix.package }}
          python -m build --wheel

      - name: Validate wheel
        run: |
          export PATH=$PATH:$HOME/.local/bin
          cd databricks/wheels/dbr-env-${{ matrix.package }}
          python -m twine check dist/*.whl

      - name: Upload wheel
        uses: actions/upload-artifact@v4
        with:
          name: wheel-${{ matrix.package }}
          path: databricks/wheels/dbr-env-${{ matrix.package }}/dist/*.whl
          retention-days: 7

  test-python-packages:
    needs: [build-wheels, build-docker-images]
    runs-on: [self-hosted, linux]
    strategy:
      matrix:
        include:
          - dbr-version: dbr15
            service-name: dbr15
          - dbr-version: dbr16
            service-name: dbr16

    steps:
      - uses: actions/checkout@v4

      - name: Download wheels
        uses: actions/download-artifact@v4
        with:
          pattern: wheel-*
          path: databricks/dist/
          merge-multiple: true

      - name: Install wheels and test
        run: |
          cd databricks
          # Create a test script to run inside the container
          cat > /tmp/test_script.sh << 'SCRIPT_END'
          set -e
          echo '=== Installing wheels ==='
          pip install --upgrade pip
          pip install dist/dbr_env_core-*.whl
          pip install dist/dbr_env_ml-*.whl
          pip install dist/dbr_env_cloud-*.whl
          pip install dist/dbr_env_all-*.whl

          echo '=== Installing DBR dependencies ==='
          pip install "dbr-env-all[__DBR_VERSION_PLACEHOLDER__]"

          echo '=== Testing imports ==='
          python -c "
          from dbr_env_core import get_dbr_info
          from dbr_env_ml import get_ml_info
          from dbr_env_cloud import get_cloud_info
          from dbr_env_all import get_all_info
          print('✓ All imports successful')

          info = get_all_info('__DBR_VERSION_PLACEHOLDER__')
          print(f'DBR Info: {info}')
          "

          echo '=== Testing mock functionality ==='
          python -c "
          from dbr_env_core.mock import get_mock_spark_session, get_mock_databricks_client

          # Test mock Spark
          spark = get_mock_spark_session('TestApp')
          df = spark.createDataFrame([{'test': 'data'}])
          print(f'Mock Spark DataFrame count: {df.count()}')

          # Test mock Databricks
          client = get_mock_databricks_client()
          clusters = client.clusters.list()
          print(f'Mock clusters: {len(clusters)}')

          print('✓ Mock tests passed')
          "

          echo '=== Running validation (should pass all checks) ==='
          python -m dbr_env_all.validate --version __DBR_VERSION_PLACEHOLDER__ --json

          echo '=== Human-readable validation ==='
          python -m dbr_env_all.validate --version __DBR_VERSION_PLACEHOLDER__
          SCRIPT_END

          # Replace placeholders with actual values
          sed -i "s/__DBR_VERSION_PLACEHOLDER__/${{ matrix.dbr-version }}/g" /tmp/test_script.sh

          # Run the script inside the container
          ./run-compose.sh run --rm -v /tmp/test_script.sh:/tmp/test_script.sh:ro ${{ matrix.service-name }} bash /tmp/test_script.sh

  test-scripts:
    runs-on: [self-hosted, linux]
    container:
      image: python:3.11-slim
      options: --user 1000:1000
    steps:
      - uses: actions/checkout@v4

      - name: Test validation script availability
        run: |
          # Note: dbr-validate is provided by the Python package, not a bash script
          # It will be available after installing the dbr-env-all package
          echo "dbr-validate is provided by the Python dbr-env-all package"

      - name: Test pre-installation script
        run: |
          databricks/scripts/dbr-setup-pre --help

      - name: Test post-installation script
        run: |
          databricks/scripts/dbr-setup-post --help

  integration-test:
    needs: [test-python-packages, test-scripts]
    runs-on: [self-hosted, linux]
    steps:
      - uses: actions/checkout@v4

      - name: Download wheels
        uses: actions/download-artifact@v4
        with:
          pattern: wheel-*
          path: databricks/dist/
          merge-multiple: true

      - name: Run integration test in DBR15 container
        run: |
          cd databricks
          # Create integration test script
          cat > /tmp/integration_test.sh << 'SCRIPT_END'
          set -e
          echo '=== Installing packages ==='
          pip install --upgrade pip
          pip install dist/*.whl
          pip install 'dbr-env-all[dbr15]'

          echo '=== Running integration test ==='
          python -c "
          from dbr_env_all import get_all_info
          from dbr_env_core.mock import get_mock_spark_session

          # Get environment info
          info = get_all_info('dbr15')
          assert 'core' in info
          assert 'ml' in info
          assert 'cloud' in info

          # Test mock Spark
          spark = get_mock_spark_session()
          df = spark.createDataFrame([
              {'name': 'test', 'value': 123}
          ])
          assert df.count() == 1

          print('✓ Integration test passed')
          "

          echo '=== Final validation in DBR15 environment ==='
          python -m dbr_env_all.validate --version dbr15 --json
          SCRIPT_END

          # Run the script inside the container
          ./run-compose.sh run --rm -v /tmp/integration_test.sh:/tmp/integration_test.sh:ro dbr15 bash /tmp/integration_test.sh
