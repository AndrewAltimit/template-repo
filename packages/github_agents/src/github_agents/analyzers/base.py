"""Base analyzer class for codebase analysis agents.

This module provides the foundation for specialized analyzers that inspect
the codebase and generate findings for automated issue creation.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import hashlib
import logging
from pathlib import Path
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)


class FindingCategory(Enum):
    """Categories of analysis findings."""

    SECURITY = "security"
    PERFORMANCE = "performance"
    QUALITY = "quality"
    TECH_DEBT = "tech_debt"
    DOCUMENTATION = "documentation"
    TESTING = "testing"
    ARCHITECTURE = "architecture"
    DEPENDENCY = "dependency"


class FindingPriority(Enum):
    """Priority levels for findings."""

    P0 = "P0"  # Critical - Security vulnerabilities, data loss risks
    P1 = "P1"  # High - Bugs, performance issues
    P2 = "P2"  # Medium - Code quality, tech debt
    P3 = "P3"  # Low - Nice to have, minor improvements


@dataclass
class AffectedFile:
    """Represents a file affected by a finding."""

    path: str
    line_start: Optional[int] = None
    line_end: Optional[int] = None
    snippet: Optional[str] = None

    def to_reference(self) -> str:
        """Generate a file reference string."""
        if self.line_start and self.line_end:
            return f"`{self.path}:L{self.line_start}-L{self.line_end}`"
        elif self.line_start:
            return f"`{self.path}:L{self.line_start}`"
        return f"`{self.path}`"


@dataclass
class AnalysisFinding:
    """Represents a single finding from codebase analysis."""

    title: str
    summary: str
    details: str
    category: FindingCategory
    priority: FindingPriority
    affected_files: List[AffectedFile]
    suggested_fix: str
    evidence: str
    discovered_by: str  # Agent name
    analysis_date: datetime = field(default_factory=datetime.utcnow)
    effort_estimate: str = "M"  # XS, S, M, L, XL
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def fingerprint(self) -> str:
        """Generate a unique fingerprint for deduplication.

        The fingerprint is based on category, affected files, and key content
        to identify conceptually similar findings across runs.
        """
        components = [
            self.category.value,
            self.priority.value,
            # Include first affected file path and line range
            self.affected_files[0].path if self.affected_files else "",
            str(self.affected_files[0].line_start) if self.affected_files else "",
            # Include key words from title (normalized)
            " ".join(sorted(self.title.lower().split()[:5])),
        ]
        content = "|".join(components)
        return hashlib.sha256(content.encode()).hexdigest()[:16]

    def to_issue_body(self) -> str:
        """Generate GitHub issue body markdown."""
        affected_refs = "\n".join(f"- {af.to_reference()}" for af in self.affected_files)

        evidence_section = ""
        if self.evidence:
            evidence_section = f"""### Evidence
{self.evidence}

"""

        return f"""> [!IMPORTANT]
> **This issue is reserved for AI agents.** It was automatically generated by
> the codebase analysis pipeline and will be implemented by an AI agent upon
> approval. **Public contributors: please do not work on this issue.**
> Check our other issues labeled `help wanted` for contribution opportunities.

---

## [{self.category.value.title()}]: {self.title}

**Category**: {self.category.value.title()}
**Priority**: {self.priority.value}
**Effort Estimate**: {self.effort_estimate}
**Discovered By**: {self.discovered_by}
**Analysis Run**: {self.analysis_date.strftime("%Y-%m-%d")}

### Summary
{self.summary}

### Details
{self.details}

### Affected Files
{affected_refs}

### Suggested Fix
{self.suggested_fix}

{evidence_section}---
*Generated by Codebase Analysis Pipeline*
*Awaiting admin review - reply with `[Approved]` to create PR*

<!-- analysis-fingerprint:{self.fingerprint()} -->
<!-- discovered-by:{self.discovered_by} -->
"""

    def to_issue_title(self) -> str:
        """Generate GitHub issue title."""
        prefix = f"[{self.category.value.title()}]"
        return f"{prefix} {self.title}"


class BaseAnalyzer(ABC):
    """Base class for codebase analyzers.

    Subclasses implement specific analysis logic for different concerns
    (security, performance, architecture, etc.).
    """

    def __init__(
        self,
        agent_name: str,
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None,
    ):
        """Initialize the analyzer.

        Args:
            agent_name: Name of the agent performing analysis
            include_paths: Glob patterns for files to include
            exclude_paths: Glob patterns for files to exclude
        """
        self.agent_name = agent_name
        self.include_paths = include_paths or ["**/*.py"]
        self.exclude_paths = exclude_paths or [
            "**/tests/**",
            "**/__pycache__/**",
            "**/node_modules/**",
            "**/.git/**",
        ]
        self._findings: List[AnalysisFinding] = []

    @property
    @abstractmethod
    def analysis_focus(self) -> str:
        """Human-readable description of what this analyzer focuses on."""
        pass

    @property
    @abstractmethod
    def supported_categories(self) -> List[FindingCategory]:
        """List of finding categories this analyzer can produce."""
        pass

    @abstractmethod
    async def analyze(self, repo_path: Path) -> List[AnalysisFinding]:
        """Perform analysis on the repository.

        Args:
            repo_path: Path to the repository root

        Returns:
            List of findings from the analysis
        """
        pass

    def _should_include_file(self, file_path: Path, repo_root: Path) -> bool:
        """Check if a file should be included in analysis.

        Args:
            file_path: Path to the file
            repo_root: Repository root for relative path calculation

        Returns:
            True if file should be analyzed
        """
        try:
            relative = file_path.relative_to(repo_root)
        except ValueError:
            return False

        # Check exclusions first
        for pattern in self.exclude_paths:
            if file_path.match(pattern):
                return False
            if relative.match(pattern):
                return False

        # Check inclusions
        for pattern in self.include_paths:
            if file_path.match(pattern):
                return True
            if relative.match(pattern):
                return True

        return False

    def _collect_files(self, repo_path: Path) -> List[Path]:
        """Collect all files to analyze.

        Args:
            repo_path: Path to repository root

        Returns:
            List of file paths to analyze
        """
        files = []
        for pattern in self.include_paths:
            for file_path in repo_path.glob(pattern):
                if file_path.is_file() and self._should_include_file(file_path, repo_path):
                    files.append(file_path)

        return sorted(set(files))

    def add_finding(self, finding: AnalysisFinding) -> None:
        """Add a finding to the results.

        Args:
            finding: The finding to add
        """
        self._findings.append(finding)
        logger.info(
            "Finding added: [%s] %s",
            finding.priority.value,
            finding.title,
        )

    def clear_findings(self) -> None:
        """Clear all findings."""
        self._findings.clear()

    @property
    def findings(self) -> List[AnalysisFinding]:
        """Get all findings."""
        return self._findings.copy()


class AgentAnalyzer(BaseAnalyzer):
    """Analyzer that delegates to an AI agent for analysis.

    This wraps an existing agent (Claude, Gemini, Codex, etc.) and
    prompts it to analyze specific aspects of the codebase.
    """

    def __init__(
        self,
        agent_name: str,
        agent_instance: Any,  # BaseAgent from agents module
        analysis_prompt: str,
        categories: List[FindingCategory],
        include_paths: Optional[List[str]] = None,
        exclude_paths: Optional[List[str]] = None,
    ):
        """Initialize agent-based analyzer.

        Args:
            agent_name: Name of the agent
            agent_instance: Instance of an AI agent
            analysis_prompt: Base prompt for analysis
            categories: Categories this analyzer handles
            include_paths: Glob patterns to include
            exclude_paths: Glob patterns to exclude
        """
        super().__init__(agent_name, include_paths, exclude_paths)
        self.agent = agent_instance
        self.analysis_prompt = analysis_prompt
        self._categories = categories

    @property
    def analysis_focus(self) -> str:
        return f"AI-powered analysis via {self.agent_name}"

    @property
    def supported_categories(self) -> List[FindingCategory]:
        return self._categories

    # Token/character limits for file content
    MAX_FILES_TO_READ = 20  # Maximum files to include content for
    MAX_CHARS_PER_FILE = 3000  # Maximum characters per file
    MAX_TOTAL_CHARS = 50000  # Maximum total content characters

    async def analyze(self, repo_path: Path) -> List[AnalysisFinding]:
        """Run AI agent analysis on the codebase.

        Args:
            repo_path: Path to the repository

        Returns:
            List of findings from AI analysis
        """
        self.clear_findings()
        files = self._collect_files(repo_path)

        if not files:
            logger.warning("No files found for analysis in %s", repo_path)
            return []

        # Build file content for the agent (with limits to prevent token overflow)
        file_contents = self._read_file_contents(files[: self.MAX_FILES_TO_READ], repo_path)

        # List remaining files (paths only) if we couldn't read all
        remaining_files = files[self.MAX_FILES_TO_READ :]
        remaining_list = ""
        if remaining_files:
            remaining_list = "\n\n## Additional Files (paths only, not included in analysis)\n"
            remaining_list += "\n".join(f"- {f.relative_to(repo_path)}" for f in remaining_files[:30])
            if len(remaining_files) > 30:
                remaining_list += f"\n... and {len(remaining_files) - 30} more files"

        prompt = f"""{self.analysis_prompt}

## Files to Analyze

{file_contents}{remaining_list}

## Output Format
For each finding, provide:
1. TITLE: Brief title
2. CATEGORY: {", ".join(c.value for c in self._categories)}
3. PRIORITY: P0, P1, P2, or P3
4. SUMMARY: 1-2 sentence description
5. DETAILS: Full explanation
6. FILES: List of affected files with line numbers
7. FIX: Suggested fix approach
8. EVIDENCE: Supporting code or metrics

Separate findings with "---"
"""

        # Execute agent using the existing generate_code method
        try:
            context = {
                "mode": "analysis",
                "repo_path": str(repo_path),
                "file_count": len(files),
                "files_with_content": len(files[: self.MAX_FILES_TO_READ]),
            }
            response = await self.agent.generate_code(prompt, context)
            findings = self._parse_agent_response(response)
            for finding in findings:
                self.add_finding(finding)
        except Exception as e:
            logger.error("Agent analysis failed: %s", e)

        return self.findings

    def _read_file_contents(self, files: List[Path], repo_path: Path) -> str:
        """Read file contents with character limits.

        Args:
            files: List of file paths to read
            repo_path: Repository root path

        Returns:
            Formatted string with file contents
        """
        contents = []
        total_chars = 0

        for file_path in files:
            if total_chars >= self.MAX_TOTAL_CHARS:
                contents.append(f"\n... truncated (reached {self.MAX_TOTAL_CHARS} char limit)")
                break

            try:
                relative_path = file_path.relative_to(repo_path)
                content = file_path.read_text(encoding="utf-8", errors="replace")

                # Truncate if too long
                if len(content) > self.MAX_CHARS_PER_FILE:
                    content = content[: self.MAX_CHARS_PER_FILE]
                    content += f"\n... (truncated at {self.MAX_CHARS_PER_FILE} chars)"

                # Check total limit
                if total_chars + len(content) > self.MAX_TOTAL_CHARS:
                    remaining = self.MAX_TOTAL_CHARS - total_chars
                    content = content[:remaining]
                    content += "\n... (truncated due to total limit)"

                contents.append(f"### {relative_path}\n```\n{content}\n```")
                total_chars += len(content)

            except Exception as e:
                logger.warning("Failed to read %s: %s", file_path, e)
                contents.append(f"### {file_path.relative_to(repo_path)}\n(could not read: {e})")

        return "\n\n".join(contents)

    def _parse_agent_response(self, response: str) -> List[AnalysisFinding]:
        """Parse agent response into findings.

        Args:
            response: Raw agent response text

        Returns:
            List of parsed findings
        """
        import re

        findings = []

        # Split response by separator
        sections = re.split(r"\n---+\n", response)

        for section in sections:
            section = section.strip()
            if not section or len(section) < 50:
                continue

            try:
                finding = self._parse_single_finding(section)
                if finding:
                    findings.append(finding)
            except Exception as e:
                logger.warning("Failed to parse finding section: %s", e)
                continue

        logger.debug("Parsed %d findings from %d chars", len(findings), len(response))
        return findings

    def _parse_single_finding(self, section: str) -> Optional[AnalysisFinding]:
        """Parse a single finding from a section of agent response.

        Args:
            section: Text section containing one finding

        Returns:
            Parsed finding or None if parsing fails
        """
        import re

        # Extract fields using patterns
        def extract_field(pattern: str, text: str, default: str = "") -> str:
            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)
            return match.group(1).strip() if match else default

        title = extract_field(r"TITLE:\s*(.+?)(?:\n|$)", section)
        if not title:
            # Try alternative formats
            title = extract_field(r"^#+\s*(.+?)$", section, "Untitled Finding")

        category_str = extract_field(r"CATEGORY:\s*(\w+)", section, "quality")
        priority_str = extract_field(r"PRIORITY:\s*(P\d)", section, "P2")
        summary = extract_field(r"SUMMARY:\s*(.+?)(?:\n\n|\nDETAILS:)", section)
        details = extract_field(r"DETAILS:\s*(.+?)(?:\n\n|\nFILES:)", section)
        files_str = extract_field(r"FILES:\s*(.+?)(?:\n\n|\nFIX:)", section)
        fix = extract_field(r"FIX:\s*(.+?)(?:\n\n|\nEVIDENCE:|$)", section)
        evidence = extract_field(r"EVIDENCE:\s*(.+?)$", section)

        # Parse category
        try:
            category = FindingCategory(category_str.lower())
        except ValueError:
            category = FindingCategory.QUALITY

        # Parse priority
        try:
            priority = FindingPriority(priority_str.upper())
        except ValueError:
            priority = FindingPriority.P2

        # Parse affected files
        affected_files = []
        file_pattern = re.compile(r"`?([^`\n]+?\.py)(?::L?(\d+))?(?:-L?(\d+))?`?")
        for match in file_pattern.finditer(files_str):
            path = match.group(1)
            line_start = int(match.group(2)) if match.group(2) else None
            line_end = int(match.group(3)) if match.group(3) else None
            affected_files.append(AffectedFile(path=path, line_start=line_start, line_end=line_end))

        if not affected_files:
            # Create a generic affected file if none parsed
            affected_files.append(AffectedFile(path="(unknown)"))

        if not title or not summary:
            return None

        return AnalysisFinding(
            title=title,
            summary=summary or details[:200] if details else "No summary provided",
            details=details or summary,
            category=category,
            priority=priority,
            affected_files=affected_files,
            suggested_fix=fix or "See details for recommendations",
            evidence=evidence or "",
            discovered_by=self.agent_name,
        )
