# Dashboard Testing Documentation

## Overview

The Sleeper Detection Dashboard includes a comprehensive testing suite with:
- Unit tests for components using mocked Streamlit components
- E2E tests using Selenium for full user interaction workflows
- Visual regression testing with AI agent integration
- Test data fixtures for realistic scenarios
- Docker-based testing for consistency

## Test Files

### Core Test Files
- **`test_streamlit_components.py`** - Unit tests for dashboard components (14 tests)
- **`test_selenium_e2e.py`** - End-to-end Selenium tests for user workflows
- **`ai_visual_analyzer.py`** - AI-powered visual regression testing
- **`fixtures.py`** - Test data generation and database setup
- **`check_database.py`** - Database health verification
- **`run_tests.sh`** - Test runner script
- **`cleanup.sh`** - Clean up test artifacts

## Running Tests

### Docker-Based Testing (Recommended)

```bash
# Run dashboard component tests in container
docker run --rm -v $(pwd):/app -w /app -e PYTHONPATH=/app python:3.11-slim bash -c \
  "pip install pytest streamlit pandas numpy plotly bcrypt --quiet && \
   python -m pytest packages/sleeper_detection/dashboard/tests/test_streamlit_components.py -v"

# Or use the python-ci container
docker-compose run --rm -e PYTHONPATH=/app python-ci \
  pytest packages/sleeper_detection/dashboard/tests/ -v
```

### Local Testing

```bash
# Navigate to tests directory
cd packages/sleeper_detection/dashboard/tests

# Run all tests
./run_tests.sh all

# Run specific test types
./run_tests.sh unit      # Unit tests only
./run_tests.sh e2e       # E2E Selenium tests
./run_tests.sh visual    # Visual regression tests

# Clean up after tests
./cleanup.sh
```

### Manual Test Execution

```bash
# Generate test data
python fixtures.py

# Run unit tests
python -m pytest test_streamlit_components.py -v

# Run E2E tests (requires Selenium and Chrome/ChromeDriver)
python -m pytest test_selenium_e2e.py -v

# Run visual analysis
python ai_visual_analyzer.py
```

## Test Results

### Current Test Status
- **Dashboard Component Tests**: 12 passed, 2 skipped
  - All component rendering tests pass
  - Authentication tests pass
  - Data processing tests pass

### Test Coverage
- **Components Tested**:
  - Overview component with multi-column layouts
  - Authentication manager with bcrypt
  - Data loader with SQLite integration
  - Cache manager functionality
  - Detection analysis components
  - Export manager
  - Model comparison
  - Time series analysis
  - Data processing utilities

## Test Data

### Default Test Users
Generated by `fixtures.py`:
- **admin/admin123** - Default administrator
- **testuser/testpass123** - Regular user
- **viewer/viewonly456** - View-only user
- **analyst/analyst789** - Admin analyst

### Test Database
The fixtures generate:
- Sample evaluation results
- Multiple model evaluations
- Historical time series data
- Test suite results
- Performance metrics

## Dependencies

### Required Python Packages
```bash
# Core dependencies
pytest
streamlit
pandas
numpy
plotly
bcrypt

# For E2E tests
selenium
webdriver-manager
Pillow
imagehash

# For visual analysis (optional)
openai  # If using AI analysis
```

## Common Issues and Solutions

### Import Errors
**Problem**: `ModuleNotFoundError: No module named 'packages'`
**Solution**: Run tests with `PYTHONPATH=/app` or from repository root

### Dashboard Container Issues
**Problem**: Container fails with read-only filesystem error
**Solution**: Create required directories before running:
```bash
mkdir -p evaluation_results
```

### Missing Dependencies
**Problem**: Tests fail due to missing packages
**Solution**: Use Docker containers or install dependencies:
```bash
pip install pytest streamlit pandas numpy plotly bcrypt
```

### Column Mocking Issues
**Problem**: `not enough values to unpack (expected 4, got 2)`
**Solution**: Fixed in latest version - mock all `st.columns()` calls correctly

## CI/CD Integration

Tests are configured in GitHub Actions but skip for draft PRs. When PR is ready for review:
- Dashboard Tests job runs automatically
- Sleeper Detection Tests job runs
- Results appear in PR checks

## Best Practices

### Writing New Tests
1. Use descriptive test names
2. Mock Streamlit components properly
3. Clean up resources in tearDown
4. Use fixtures for consistent test data

### Test Organization
1. Keep unit tests separate from E2E tests
2. Group related tests in classes
3. Use appropriate assertions
4. Document complex test scenarios

### Performance
1. Mock external dependencies
2. Use Docker for consistency
3. Parallelize where possible
4. Clean up test artifacts

## Extending Tests

### Adding New Component Tests
```python
def test_new_component(self):
    """Test new dashboard component."""
    from components.new_component import render_new_component

    with patch("components.new_component.st") as mock_st:
        # Mock Streamlit functions
        mock_st.write.return_value = None

        # Should not raise exceptions
        render_new_component(self.mock_data_loader, self.mock_cache_manager)
```

### Adding E2E Tests
```python
def test_new_workflow(self):
    """Test new user workflow."""
    self.login("testuser", "testpass123")

    # Navigate to feature
    self.driver.find_element(By.LINK_TEXT, "New Feature").click()

    # Verify functionality
    assert "Expected Result" in self.driver.page_source
```

## Maintenance

### Updating Test Data
1. Modify `fixtures.py` to generate new test scenarios
2. Run `python fixtures.py` to regenerate databases
3. Commit updated fixture generation code (not databases)

### Updating Baselines
For visual regression tests:
```bash
rm -rf baselines/*.png
python ai_visual_analyzer.py  # Creates new baselines
```

## Support

For testing issues:
1. Check test output for specific errors
2. Verify all dependencies are installed
3. Ensure Docker is running if using containers
4. Check that required directories exist
