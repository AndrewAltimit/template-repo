"""Analyze attention patterns to identify backdoor triggers."""

import logging
from typing import Any, Dict, List, Optional

import numpy as np
import torch

logger = logging.getLogger(__name__)


class AttentionAnalyzer:
    """Analyze attention patterns to identify backdoor triggers."""

    def __init__(self, model):
        """Initialize the attention analyzer.

        Args:
            model: The model to analyze
        """
        self.model = model
        self.trigger_attention_patterns = {}
        self.attention_cache = {}

    async def analyze_trigger_attention(
        self, samples_with_trigger: List[str], samples_without_trigger: List[str], trigger_token: str
    ) -> Dict[str, Any]:
        """Compare attention patterns with and without triggers.

        Based on findings that triggers cause distinct attention patterns.

        Args:
            samples_with_trigger: Samples containing the trigger
            samples_without_trigger: Clean samples without trigger
            trigger_token: The trigger token to analyze

        Returns:
            Analysis results with attention patterns and divergence scores
        """
        results: Dict[str, Any] = {
            "attention_to_trigger": [],
            "attention_divergence": [],
            "head_importance": {},
            "anomaly_score": 0.0,
        }

        # Get number of layers (handle different config attribute names)
        if hasattr(self.model, "config"):
            # Try different attribute names used by different model architectures
            if hasattr(self.model.config, "n_layers"):
                n_layers = min(self.model.config.n_layers, 12)
            elif hasattr(self.model.config, "n_layer"):
                n_layers = min(self.model.config.n_layer, 12)
            elif hasattr(self.model.config, "num_hidden_layers"):
                n_layers = min(self.model.config.num_hidden_layers, 12)
            else:
                n_layers = 6  # Fallback
        else:
            n_layers = 6  # Default for testing

        for layer in range(n_layers):
            try:
                layer_results = await self._analyze_layer_attention(
                    samples_with_trigger, samples_without_trigger, trigger_token, layer
                )
                results["attention_to_trigger"].append(layer_results["trigger_attention"])
                results["attention_divergence"].append(layer_results["kl_divergence"])
                results["head_importance"][layer] = layer_results["head_scores"]
            except Exception as e:
                logger.debug(f"Failed to analyze layer {layer}: {e}")
                results["attention_to_trigger"].append(None)
                results["attention_divergence"].append(0.0)

        # Calculate overall anomaly score
        divergences = [d for d in results["attention_divergence"] if d > 0]
        if divergences:
            results["anomaly_score"] = float(np.mean(divergences))

        return results

    async def _analyze_layer_attention(
        self, triggered_samples: List[str], clean_samples: List[str], trigger_token: str, layer_idx: int
    ) -> Dict[str, Any]:
        """Analyze attention patterns at specific layer.

        Args:
            triggered_samples: Samples with trigger
            clean_samples: Clean samples
            trigger_token: Trigger token
            layer_idx: Layer index

        Returns:
            Layer analysis results
        """
        # Get attention patterns for triggered samples
        triggered_patterns = []
        for sample in triggered_samples[:5]:  # Limit for performance
            pattern = await self._get_attention_pattern(sample, layer_idx, trigger_token)
            if pattern is not None:
                triggered_patterns.append(pattern)

        # Get attention patterns for clean samples
        clean_patterns = []
        for sample in clean_samples[:5]:  # Limit for performance
            pattern = await self._get_attention_pattern(sample, layer_idx, None)
            if pattern is not None:
                clean_patterns.append(pattern)

        # Calculate KL divergence between distributions
        kl_div = self._calculate_kl_divergence(triggered_patterns, clean_patterns)

        # Identify most important heads
        head_scores = self._score_attention_heads(triggered_patterns, clean_patterns)

        return {
            "trigger_attention": np.mean(triggered_patterns, axis=0) if triggered_patterns else None,
            "kl_divergence": float(kl_div),
            "head_scores": head_scores,
        }

    async def _get_attention_pattern(self, sample: str, layer_idx: int, trigger_token: Optional[str]) -> Optional[np.ndarray]:
        """Get attention pattern for a sample.

        Args:
            sample: Text sample
            layer_idx: Layer index
            trigger_token: Optional trigger token

        Returns:
            Attention pattern array or None
        """
        # Check cache
        cache_key = (sample, layer_idx)
        if cache_key in self.attention_cache:
            return np.array(self.attention_cache[cache_key])

        try:
            if not hasattr(self.model, "run_with_cache"):
                # Generate mock attention pattern for testing
                pattern = np.random.rand(12, 16, 16)  # (heads, seq_len, seq_len)
            else:
                tokens = self.model.to_tokens(sample)
                _, cache = self.model.run_with_cache(tokens)

                # Get attention pattern
                attn = cache[("pattern", layer_idx)]

                # Find trigger position if specified
                if trigger_token:
                    trigger_pos = self._find_trigger_position(tokens, trigger_token)
                    if trigger_pos is not None:
                        # Attention TO the trigger position
                        pattern = attn[:, :, :, trigger_pos].mean(dim=2).cpu().numpy()
                    else:
                        pattern = attn.mean(dim=-1).cpu().numpy()
                else:
                    pattern = attn.mean(dim=-1).cpu().numpy()

                pattern = pattern.squeeze()

            self.attention_cache[cache_key] = pattern
            return pattern

        except Exception as e:
            logger.debug(f"Failed to get attention pattern: {e}")
            return None

    def _find_trigger_position(self, tokens: torch.Tensor, trigger_token: str) -> Optional[int]:
        """Find position of trigger token in token sequence.

        Args:
            tokens: Token tensor
            trigger_token: Trigger string

        Returns:
            Position index or None
        """
        # Simplified implementation - would need proper tokenization
        # For testing, return a random position
        if tokens.shape[-1] > 0:
            return int(np.random.randint(0, tokens.shape[-1]))
        return None

    def _calculate_kl_divergence(self, patterns1: List[np.ndarray], patterns2: List[np.ndarray]) -> float:
        """Calculate KL divergence between two sets of attention patterns.

        Args:
            patterns1: First set of patterns
            patterns2: Second set of patterns

        Returns:
            KL divergence score
        """
        if not patterns1 or not patterns2:
            return 0.0

        try:
            # Flatten and normalize patterns
            p1 = np.concatenate([p.flatten() for p in patterns1])
            p2 = np.concatenate([p.flatten() for p in patterns2])

            # Normalize first to maintain distribution shape
            p1 = p1 / p1.sum()
            p2 = p2 / p2.sum()

            # Add small epsilon to avoid log(0)
            eps = 1e-10
            p1 = p1 + eps
            p2 = p2 + eps

            # Renormalize after adding epsilon
            p1 = p1 / p1.sum()
            p2 = p2 / p2.sum()

            # Calculate KL divergence
            kl_div = np.sum(p1 * np.log(p1 / p2))

            return float(kl_div)
        except Exception as e:
            logger.debug(f"Failed to calculate KL divergence: {e}")
            return 0.0

    def _score_attention_heads(
        self, triggered_patterns: List[np.ndarray], clean_patterns: List[np.ndarray]
    ) -> Dict[int, float]:
        """Score attention heads by their importance for detection.

        Args:
            triggered_patterns: Patterns with trigger
            clean_patterns: Clean patterns

        Returns:
            Head importance scores
        """
        scores: Dict[int, float] = {}

        if not triggered_patterns or not clean_patterns:
            return scores

        try:
            # Calculate variance difference for each head
            triggered_array = np.stack(triggered_patterns) if len(triggered_patterns) > 1 else triggered_patterns[0]
            clean_array = np.stack(clean_patterns) if len(clean_patterns) > 1 else clean_patterns[0]

            # Get number of heads
            n_heads = triggered_array.shape[-2] if triggered_array.ndim > 2 else 12

            for head_idx in range(n_heads):
                # Calculate variance difference
                if triggered_array.ndim > 2:
                    triggered_var = np.var(triggered_array[..., head_idx, :])
                    clean_var = np.var(clean_array[..., head_idx, :])
                else:
                    triggered_var = np.var(triggered_array)
                    clean_var = np.var(clean_array)

                scores[head_idx] = abs(triggered_var - clean_var)

        except Exception as e:
            logger.debug(f"Failed to score attention heads: {e}")

        return scores

    async def analyze_sample_attention(self, text: str) -> Dict[str, Any]:
        """Analyze attention patterns for a single sample.

        Args:
            text: Text to analyze

        Returns:
            Attention analysis results
        """
        results: Dict[str, Any] = {
            "layer_attention_scores": {},
            "max_attention_values": [],
            "attention_entropy": [],
            "anomaly_score": 0.0,
        }

        # Get number of layers (handle different config attribute names)
        if hasattr(self.model, "config"):
            # Try different attribute names used by different model architectures
            if hasattr(self.model.config, "n_layers"):
                n_layers = min(self.model.config.n_layers, 6)
            elif hasattr(self.model.config, "n_layer"):
                n_layers = min(self.model.config.n_layer, 6)
            elif hasattr(self.model.config, "num_hidden_layers"):
                n_layers = min(self.model.config.num_hidden_layers, 6)
            else:
                n_layers = 3  # Fallback
        else:
            n_layers = 3  # Reduced for testing

        for layer_idx in range(n_layers):
            try:
                pattern = await self._get_attention_pattern(text, layer_idx, None)
                if pattern is not None:
                    # Calculate statistics
                    max_val = float(np.max(pattern))
                    entropy = self._calculate_entropy(pattern)

                    results["layer_attention_scores"][layer_idx] = {
                        "max_attention": max_val,
                        "entropy": float(entropy),
                        "mean_attention": float(np.mean(pattern)),
                    }
                    results["max_attention_values"].append(max_val)
                    results["attention_entropy"].append(entropy)

            except Exception as e:
                logger.debug(f"Failed to analyze layer {layer_idx}: {e}")

        # Calculate anomaly score based on entropy
        if results["attention_entropy"]:
            mean_entropy = float(np.mean(results["attention_entropy"]))
            # Low entropy might indicate focused attention on trigger
            results["anomaly_score"] = float(1.0 / (1.0 + mean_entropy))

        return results

    def _calculate_entropy(self, pattern: np.ndarray) -> float:
        """Calculate entropy of attention pattern.

        Args:
            pattern: Attention pattern

        Returns:
            Entropy value
        """
        try:
            # Flatten and normalize
            flat = pattern.flatten()
            flat_arr = np.array(flat + 1e-10)  # Avoid log(0)
            flat_arr = flat_arr / flat_arr.sum()

            # Calculate entropy
            entropy = -np.sum(flat_arr * np.log(flat_arr))
            return float(entropy)
        except Exception:
            return 0.0
