
\maketitle
\tableofcontents
\newpage

\section{Introduction}

This comprehensive guide provides practical troubleshooting solutions for common issues encountered when using the Sleeper Agents Detection Framework. Each problem includes a clear description, root cause analysis, step-by-step solutions, and prevention strategies.

\begin{warningbox}{Quick Reference}
For urgent issues:
\begin{itemize}
    \item GPU not detected: See Section \ref{sec:gpu-not-detected}
    \item Out of memory: See Section \ref{sec:oom-runtime}
    \item Dashboard won't start: See Section \ref{sec:dashboard-wont-start}
    \item Slow performance: See Section \ref{sec:slow-performance}
\end{itemize}
\end{warningbox}

\section{Installation \& Setup Issues}

\subsection{GPU Not Detected / CUDA Errors}
\label{sec:gpu-not-detected}

\begin{problembox}{Problem: CUDA not available or GPU not detected}
\textbf{Symptoms:}
\begin{itemize}
    \item Error: \texttt{RuntimeError: CUDA out of memory}
    \item Warning: \texttt{CUDA not available, using CPU}
    \item \texttt{torch.cuda.is\_available()} returns \texttt{False}
    \item Model runs extremely slowly
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} CUDA drivers not installed, incompatible PyTorch version, or Docker configuration issue.

\textbf{Step-by-Step Fix:}

\textbf{1. Verify CUDA Installation}
\begin{lstlisting}[language=bash]
# Check NVIDIA driver
nvidia-smi

# Check CUDA version
nvcc --version

# Test PyTorch CUDA availability
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}'); print(f'Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else "None"}')"
\end{lstlisting}

\textbf{2. Install Compatible PyTorch}
\begin{lstlisting}[language=bash]
# For CUDA 11.8
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Verify installation
python -c "import torch; print(torch.version.cuda)"
\end{lstlisting}

\textbf{3. Fix Docker GPU Access}
\begin{lstlisting}[language=bash]
# Install NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker

# Test GPU in Docker
docker run --rm --gpus all nvidia/cuda:11.8.0-base nvidia-smi
\end{lstlisting}

\textbf{4. Configure Environment}
\begin{lstlisting}[language=bash]
# Add to .bashrc or .env
export CUDA_VISIBLE_DEVICES=0
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
\end{lstlisting}
\end{solutionbox}

\begin{warningbox}{Prevention}
\begin{itemize}
    \item Always match PyTorch CUDA version with system CUDA version
    \item Test GPU availability before large evaluations: \texttt{python packages/sleeper\_agents/scripts/test\_cpu\_mode.py}
    \item Use containerized evaluation for consistent GPU access
    \item Document your CUDA version in project README
\end{itemize}
\end{warningbox}

\subsection{Docker Permission Issues}

\begin{problembox}{Problem: Permission denied errors in Docker}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{Permission denied} when writing results
    \item \texttt{PermissionError: [Errno 13] Permission denied: '/results/evaluation.db'}
    \item Results directory empty after evaluation
    \item Cannot delete or modify result files
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Docker container running as root, creating files owned by root instead of host user.

\textbf{Step-by-Step Fix:}

\textbf{1. Fix Existing Permission Issues}
\begin{lstlisting}[language=bash]
# Fix permissions on results directory
sudo chown -R $(id -u):$(id -g) evaluation_results/
sudo chmod -R 755 evaluation_results/

# Or use the provided script
./automation/setup/runner/fix-runner-permissions.sh
\end{lstlisting}

\textbf{2. Run Docker with User Permissions}
\begin{lstlisting}[language=bash]
# Single command execution
docker run --rm \
  --user $(id -u):$(id -g) \
  -v $(pwd)/evaluation_results:/results \
  sleeper-eval-cpu \
  python -m packages.sleeper_agents.cli evaluate gpt2

# Docker Compose - add to service definition
docker-compose run --user $(id -u):$(id -g) sleeper-eval-cpu
\end{lstlisting}

\textbf{3. Update docker-compose.yml}
\begin{lstlisting}[language=yaml]
services:
  sleeper-eval-cpu:
    user: "${UID:-1000}:${GID:-1000}"
    volumes:
      - ./evaluation_results:/results
    environment:
      - HOME=/tmp
\end{lstlisting}

\textbf{4. Create Directories with Correct Permissions}
\begin{lstlisting}[language=bash]
# Create directories before Docker run
mkdir -p evaluation_results model_cache database
chmod 755 evaluation_results model_cache database
\end{lstlisting}
\end{solutionbox}

\subsection{Package Dependency Conflicts}

\begin{problembox}{Problem: Conflicting package versions or import errors}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{ImportError: cannot import name 'X' from 'Y'}
    \item \texttt{ModuleNotFoundError: No module named 'transformers'}
    \item \texttt{AttributeError: module 'torch' has no attribute 'X'}
    \item Version mismatch warnings
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Incompatible package versions, outdated dependencies, or environment conflicts.

\textbf{Step-by-Step Fix:}

\textbf{1. Use Clean Virtual Environment}
\begin{lstlisting}[language=bash]
# Create fresh virtual environment
python -m venv sleeper_env
source sleeper_env/bin/activate  # Linux/Mac
# sleeper_env\Scripts\activate  # Windows

# Upgrade pip
pip install --upgrade pip setuptools wheel
\end{lstlisting}

\textbf{2. Install with Exact Requirements}
\begin{lstlisting}[language=bash]
# CPU-only installation
pip install -r config/python/requirements-sleeper-agents.txt

# GPU installation (after CPU install)
pip install torch --index-url https://download.pytorch.org/whl/cu118
pip install -r config/python/requirements-sleeper-agents-gpu.txt

# Install package in editable mode
pip install -e packages/sleeper_agents
\end{lstlisting}

\textbf{3. Verify Installation}
\begin{lstlisting}[language=bash]
# Check critical packages
pip list | grep -E "torch|transformers|transformer-lens|einops"

# Expected versions:
# torch>=2.0.0
# transformers>=4.35.0
# transformer-lens>=2.0.0
# einops>=0.7.0

# Test imports
python -c "from packages.sleeper_agents.app.detector import SleeperDetector; print('Success')"
\end{lstlisting}

\textbf{4. Resolve Specific Conflicts}
\begin{lstlisting}[language=bash]
# Transformers version conflict
pip install transformers==4.35.2 --force-reinstall

# TransformerLens compatibility
pip install git+https://github.com/neelnanda-io/TransformerLens.git

# Torch version mismatch
pip uninstall torch torchvision torchaudio
pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0
\end{lstlisting}
\end{solutionbox}

\subsection{Database Connection Errors}

\begin{problembox}{Problem: Cannot connect to results database}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{sqlite3.OperationalError: unable to open database file}
    \item \texttt{PermissionError: [Errno 13] Permission denied: 'evaluation.db'}
    \item Database locked errors
    \item Results not persisting across evaluations
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Missing database directory, permission issues, or concurrent access conflicts.

\textbf{Step-by-Step Fix:}

\textbf{1. Create Database Directory}
\begin{lstlisting}[language=bash]
# Create database directory with permissions
mkdir -p database
chmod 755 database

# Set environment variable
export EVAL_DB_PATH=$(pwd)/database/evaluation.db

# Add to .env file
echo "EVAL_DB_PATH=$(pwd)/database/evaluation.db" >> .env
\end{lstlisting}

\textbf{2. Fix Database Permissions}
\begin{lstlisting}[language=bash]
# Fix database file permissions
chmod 664 database/evaluation.db
chmod 755 database/

# In Docker, ensure correct ownership
docker run --user $(id -u):$(id -g) \
  -v $(pwd)/database:/db \
  -e EVAL_DB_PATH=/db/evaluation.db \
  sleeper-eval-cpu
\end{lstlisting}

\textbf{3. Resolve Database Locks}
\begin{lstlisting}[language=bash]
# Check for lock file
ls -la database/*.db-journal database/*.db-shm database/*.db-wal

# Remove stale locks (ensure no processes using DB first)
rm -f database/evaluation.db-journal
rm -f database/evaluation.db-shm
rm -f database/evaluation.db-wal

# Or reset database entirely
rm database/evaluation.db
# Will be recreated on next run
\end{lstlisting}

\textbf{4. Test Database Connection}
\begin{lstlisting}[language=python]
# test_db.py
import sqlite3
import os

db_path = os.getenv('EVAL_DB_PATH', 'database/evaluation.db')
try:
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    print(f"Database connected: {db_path}")
    print(f"Tables: {cursor.fetchall()}")
    conn.close()
    print("Success!")
except Exception as e:
    print(f"Error: {e}")
\end{lstlisting}
\end{solutionbox}

\subsection{Dashboard Won't Start}
\label{sec:dashboard-wont-start}

\begin{problembox}{Problem: Dashboard fails to launch or becomes unresponsive}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{streamlit: command not found}
    \item \texttt{ModuleNotFoundError: No module named 'streamlit'}
    \item Dashboard starts but shows blank page
    \item Port already in use errors
    \item Authentication failures
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Missing dashboard dependencies, port conflicts, or authentication configuration issues.

\textbf{Step-by-Step Fix:}

\textbf{1. Install Dashboard Dependencies}
\begin{lstlisting}[language=bash]
# Install dashboard requirements
pip install -r packages/sleeper_agents/dashboard/requirements.txt

# Key dependencies:
# - streamlit>=1.28.0
# - plotly>=5.17.0
# - pandas>=2.0.0
# - numpy>=1.24.0

# Verify streamlit
streamlit --version
\end{lstlisting}

\textbf{2. Fix Port Conflicts}
\begin{lstlisting}[language=bash]
# Check if port 8501 is in use
lsof -i :8501
netstat -tulpn | grep 8501

# Kill existing process
kill $(lsof -t -i:8501)

# Or use different port
streamlit run packages/sleeper_agents/dashboard/app.py --server.port=8502
\end{lstlisting}

\textbf{3. Launch Dashboard Correctly}
\begin{lstlisting}[language=bash]
# Using launcher script (recommended)
./packages/sleeper_agents/dashboard/start.sh

# Select option 1 (mock data) for testing
# Select option 1 (Docker) or 2 (Local) for deployment

# Or manual launch
cd packages/sleeper_agents/dashboard
streamlit run app.py

# With custom configuration
streamlit run app.py --server.port=8501 --server.address=0.0.0.0
\end{lstlisting}

\textbf{4. Configure Authentication}
\begin{lstlisting}[language=bash]
# Default credentials for mock mode:
# Username: admin
# Password: admin123

# To customize, edit dashboard/config.py
# Or set environment variables
export DASHBOARD_USERNAME=myuser
export DASHBOARD_PASSWORD=mypassword
\end{lstlisting}

\textbf{5. Check Dashboard Logs}
\begin{lstlisting}[language=bash]
# View streamlit logs
tail -f ~/.streamlit/logs/*.log

# Enable debug logging
streamlit run app.py --logger.level=debug
\end{lstlisting}
\end{solutionbox}

\subsection{Port Conflicts}

\begin{problembox}{Problem: Required ports already in use}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{OSError: [Errno 98] Address already in use}
    \item \texttt{Port 8501 is already in use}
    \item Dashboard or API won't start
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Identify Process Using Port}
\begin{lstlisting}[language=bash]
# Linux/Mac
lsof -i :8501
netstat -tulpn | grep 8501

# Windows
netstat -ano | findstr :8501
\end{lstlisting}

\textbf{2. Kill Process or Change Port}
\begin{lstlisting}[language=bash]
# Kill process
kill $(lsof -t -i:8501)

# Or use alternative port
streamlit run app.py --server.port=8502

# For API server (default 8021)
export API_PORT=8022
python -m packages.sleeper_agents.api.server
\end{lstlisting}
\end{solutionbox}

\subsection{Memory Errors During Installation}

\begin{problembox}{Problem: Out of memory during package installation}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{Killed} during pip install
    \item System becomes unresponsive
    \item Installation fails partway through
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Increase Swap Space}
\begin{lstlisting}[language=bash]
# Check current swap
free -h

# Create swap file (4GB)
sudo fallocate -l 4G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Make permanent
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
\end{lstlisting}

\textbf{2. Install with Limited Concurrency}
\begin{lstlisting}[language=bash]
# Install packages one at a time
pip install --no-cache-dir torch
pip install --no-cache-dir transformers
pip install --no-cache-dir transformer-lens

# Or with memory limits (Docker)
docker build --memory=4g --memory-swap=4g -f docker/sleeper-evaluation-cpu.Dockerfile .
\end{lstlisting}
\end{solutionbox}

\section{Runtime Issues}

\subsection{Model Loading Failures}

\begin{problembox}{Problem: Cannot load model from HuggingFace}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{OSError: [model] does not appear to be a valid repository}
    \item \texttt{HTTPError: 401 Client Error: Unauthorized}
    \item \texttt{ConnectionError: [Errno 111] Connection refused}
    \item Model downloads extremely slowly or fails
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Invalid model name, authentication required, network issues, or insufficient disk space.

\textbf{Step-by-Step Fix:}

\textbf{1. Verify Model Name}
\begin{lstlisting}[language=bash]
# Common model names for testing:
# - EleutherAI/pythia-70m (tiny, fast)
# - gpt2 (small, reliable)
# - distilgpt2 (small, fast)
# - EleutherAI/pythia-1b
# - meta-llama/Llama-2-7b-hf (requires auth)

# Test model availability
python -c "from transformers import AutoModel; AutoModel.from_pretrained('gpt2')"
\end{lstlisting}

\textbf{2. Configure HuggingFace Authentication}
\begin{lstlisting}[language=bash]
# Get token from https://huggingface.co/settings/tokens
huggingface-cli login

# Or set environment variable
export HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx

# Add to .env file
echo "HUGGINGFACE_TOKEN=hf_xxxxxxxxxxxxx" >> .env

# Test authenticated access
python -c "from transformers import AutoModel; AutoModel.from_pretrained('meta-llama/Llama-2-7b-hf', use_auth_token=True)"
\end{lstlisting}

\textbf{3. Configure Model Cache}
\begin{lstlisting}[language=bash]
# Set cache directories
export TRANSFORMERS_CACHE=/path/to/large/disk/models
export HF_HOME=/path/to/large/disk/models
export TORCH_HOME=/path/to/large/disk/models

# Create cache directory
mkdir -p /path/to/large/disk/models
chmod 755 /path/to/large/disk/models

# Check available disk space (need 50GB+ for large models)
df -h /path/to/large/disk
\end{lstlisting}

\textbf{4. Use Offline Mode with Pre-downloaded Models}
\begin{lstlisting}[language=bash]
# Download model once
python -c "from transformers import AutoModel, AutoTokenizer; \
AutoModel.from_pretrained('gpt2'); \
AutoTokenizer.from_pretrained('gpt2')"

# Enable offline mode
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1

# Now can run without internet
python -m packages.sleeper_agents.cli evaluate gpt2
\end{lstlisting}
\end{solutionbox}

\subsection{Out of Memory Errors During Evaluation}
\label{sec:oom-runtime}

\begin{problembox}{Problem: CUDA/CPU out of memory during model evaluation}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{RuntimeError: CUDA out of memory}
    \item \texttt{MemoryError: Unable to allocate tensor}
    \item System freezes during evaluation
    \item Process killed by OS
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Model too large for available GPU/RAM, excessive batch size, or memory leaks.

\textbf{Step-by-Step Fix:}

\textbf{1. Reduce Model Size or Use Quantization}
\begin{lstlisting}[language=python]
# Use smaller model for testing
from packages.sleeper_agents.app.detector import SleeperDetector, DetectionConfig

config = DetectionConfig(
    model_name="EleutherAI/pythia-70m",  # Smallest model
    use_minimal_model=True,
    device="cuda",
    quantization="8bit"  # Or "4bit" for maximum savings
)

detector = SleeperDetector(config)
\end{lstlisting}

\textbf{2. Reduce Batch Size}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="gpt2",
    batch_size=1,  # Reduce from default (usually 4-8)
    gradient_checkpointing=True  # Save memory during backprop
)
\end{lstlisting}

\textbf{3. Clear GPU Cache Between Operations}
\begin{lstlisting}[language=python]
import torch

# Clear GPU cache
torch.cuda.empty_cache()

# Enable automatic memory management
torch.cuda.set_per_process_memory_fraction(0.8)  # Use max 80% GPU memory
\end{lstlisting}

\textbf{4. Use CPU Mode for Large Models}
\begin{lstlisting}[language=bash]
# Force CPU mode (slower but won't OOM)
export CUDA_VISIBLE_DEVICES=""

python -m packages.sleeper_agents.cli evaluate gpt2 --cpu

# Or use Docker CPU image
docker-compose run sleeper-eval-cpu python -m packages.sleeper_agents.cli evaluate gpt2
\end{lstlisting}

\textbf{5. Increase System Resources}
\begin{lstlisting}[language=bash]
# Check memory usage
nvidia-smi  # GPU
free -h     # RAM

# Increase Docker memory limits
docker run --memory="16g" --memory-swap="16g" --gpus all sleeper-eval-gpu

# Or in docker-compose.yml
services:
  sleeper-eval-gpu:
    deploy:
      resources:
        limits:
          memory: 16G
\end{lstlisting}
\end{solutionbox}

\subsection{Slow Performance / Low GPU Utilization}
\label{sec:slow-performance}

\begin{problembox}{Problem: Evaluation takes extremely long or GPU underutilized}
\textbf{Symptoms:}
\begin{itemize}
    \item GPU utilization below 30\%
    \item Evaluation taking hours instead of minutes
    \item CPU maxed out despite GPU available
    \item Multiple CPU cores idle
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Running on CPU instead of GPU, small batch size, data loading bottleneck, or incorrect configuration.

\textbf{Step-by-Step Fix:}

\textbf{1. Verify GPU is Being Used}
\begin{lstlisting}[language=bash]
# Monitor GPU during evaluation
watch -n 1 nvidia-smi

# Check PyTorch device
python -c "import torch; from packages.sleeper_agents.app.detector import SleeperDetector; \
print(f'Using device: {torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")}')"
\end{lstlisting}

\textbf{2. Increase Batch Size}
\begin{lstlisting}[language=python]
# Larger batch size = better GPU utilization
config = DetectionConfig(
    model_name="gpt2",
    batch_size=16,  # Increase if memory allows
    device="cuda"
)

# Test different batch sizes
for batch_size in [4, 8, 16, 32]:
    config.batch_size = batch_size
    # Run and time evaluation
\end{lstlisting}

\textbf{3. Enable Multi-threading for Data Loading}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="gpt2",
    num_workers=4,  # Parallel data loading
    pin_memory=True,  # Faster host->GPU transfer
    prefetch_factor=2  # Prefetch batches
)
\end{lstlisting}

\textbf{4. Use Mixed Precision Training}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="gpt2",
    use_fp16=True,  # Half precision (faster on modern GPUs)
    device="cuda"
)
\end{lstlisting}

\textbf{5. Optimize Model Configuration}
\begin{lstlisting}[language=bash]
# Use optimized model versions
python -m packages.sleeper_agents.cli evaluate gpt2 \
  --gpu \
  --batch-size 16 \
  --fp16 \
  --compile  # PyTorch 2.0+ compilation
\end{lstlisting}
\end{solutionbox}

\subsection{Detection Methods Timing Out}

\begin{problembox}{Problem: Individual detection methods hang or timeout}
\textbf{Symptoms:}
\begin{itemize}
    \item Evaluation stuck on specific detection method
    \item \texttt{TimeoutError: Detection method exceeded time limit}
    \item Progress bar frozen
    \item No output for extended period
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Increase Timeout Limits}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="gpt2",
    method_timeout=600,  # Increase from default (usually 300s)
    max_samples=100  # Reduce number of test samples
)
\end{lstlisting}

\textbf{2. Run Methods Individually}
\begin{lstlisting}[language=bash]
# Test each method separately to identify bottleneck
python -m packages.sleeper_agents.cli evaluate gpt2 \
  --suites basic

python -m packages.sleeper_agents.cli evaluate gpt2 \
  --suites code_vulnerability

python -m packages.sleeper_agents.cli evaluate gpt2 \
  --suites robustness
\end{lstlisting}

\textbf{3. Enable Debug Logging}
\begin{lstlisting}[language=python]
import logging

logging.basicConfig(level=logging.DEBUG)
# Will show progress and identify where hanging occurs
\end{lstlisting}
\end{solutionbox}

\subsection{Probe Training Convergence Issues}

\begin{problembox}{Problem: Linear probes fail to train or show poor performance}
\textbf{Symptoms:}
\begin{itemize}
    \item Probe accuracy stuck at 50\% (random)
    \item Loss not decreasing during training
    \item \texttt{NaN} loss values
    \item Extremely low AUROC scores (<0.6)
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Insufficient training data, improper normalization, learning rate issues, or data imbalance.

\textbf{Step-by-Step Fix:}

\textbf{1. Verify Training Data Quality}
\begin{lstlisting}[language=python]
# Check data distribution
from packages.sleeper_agents.probes.training import ProbeTrainer

trainer = ProbeTrainer(config)
data_stats = trainer.analyze_data()
print(f"Positive samples: {data_stats['positive_count']}")
print(f"Negative samples: {data_stats['negative_count']}")
print(f"Class balance: {data_stats['balance_ratio']}")

# Need at least 100 samples per class, ideally 1000+
# Balance ratio should be between 0.3 and 3.0
\end{lstlisting}

\textbf{2. Adjust Training Hyperparameters}
\begin{lstlisting}[language=python]
probe_config = {
    'learning_rate': 1e-3,  # Try 1e-2, 1e-3, 1e-4
    'num_epochs': 100,  # Increase if underfitting
    'early_stopping_patience': 10,
    'weight_decay': 0.01,  # L2 regularization
    'class_weights': 'balanced'  # Handle imbalanced data
}

trainer = ProbeTrainer(config, probe_config)
results = trainer.train()
\end{lstlisting}

\textbf{3. Check for NaN/Inf Values}
\begin{lstlisting}[language=python]
import torch

# Add gradient clipping
trainer.gradient_clip_norm = 1.0

# Verify activations are valid
def check_activations(activations):
    if torch.isnan(activations).any():
        print("WARNING: NaN in activations!")
    if torch.isinf(activations).any():
        print("WARNING: Inf in activations!")
    return activations
\end{lstlisting}

\textbf{4. Use Different Probe Architecture}
\begin{lstlisting}[language=python]
probe_config = {
    'architecture': 'mlp',  # Try 'linear', 'mlp', or 'deep'
    'hidden_dims': [256, 128],  # For MLP
    'dropout': 0.1,
    'activation': 'relu'
}
\end{lstlisting}
\end{solutionbox}

\subsection{Dashboard Connection Errors}

\begin{problembox}{Problem: Dashboard cannot connect to backend or load results}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{ConnectionError: Failed to connect to database}
    \item Empty results in dashboard
    \item Stale data shown
    \item Charts not updating
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Verify Database Path}
\begin{lstlisting}[language=bash]
# Check environment variable
echo $EVAL_DB_PATH

# Should point to valid database file
ls -lh $EVAL_DB_PATH

# Or use default location
ls -lh database/evaluation.db
\end{lstlisting}

\textbf{2. Refresh Dashboard}
\begin{lstlisting}[language=bash]
# Restart dashboard
pkill -f streamlit
./packages/sleeper_agents/dashboard/start.sh

# Clear Streamlit cache
rm -rf ~/.streamlit/cache
\end{lstlisting}

\textbf{3. Verify Results Exist}
\begin{lstlisting}[language=bash]
# List evaluation results
python -m packages.sleeper_agents.cli list --models
python -m packages.sleeper_agents.cli list --results

# Check database content
sqlite3 database/evaluation.db "SELECT COUNT(*) FROM evaluations;"
\end{lstlisting}
\end{solutionbox}

\subsection{Results Not Saving to Database}

\begin{problembox}{Problem: Evaluation completes but results not persisted}
\textbf{Symptoms:}
\begin{itemize}
    \item Results show during evaluation but disappear
    \item Database empty after evaluation
    \item Cannot generate reports
    \item \texttt{list --results} shows nothing
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Check Database Permissions}
\begin{lstlisting}[language=bash]
# Verify write permissions
ls -la database/
touch database/test.txt && rm database/test.txt || echo "No write permission!"

# Fix permissions
chmod 755 database/
chmod 664 database/evaluation.db
\end{lstlisting}

\textbf{2. Verify Database Path Configuration}
\begin{lstlisting}[language=bash]
# Check environment variable
export EVAL_DB_PATH=$(pwd)/database/evaluation.db

# Verify in Python
python -c "import os; print(f'DB Path: {os.getenv(\"EVAL_DB_PATH\")}')"
\end{lstlisting}

\textbf{3. Test Database Write}
\begin{lstlisting}[language=python]
# test_db_write.py
import sqlite3
import os

db_path = os.getenv('EVAL_DB_PATH', 'database/evaluation.db')
conn = sqlite3.connect(db_path)
cursor = conn.cursor()

# Try to write test record
try:
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS test (id INTEGER PRIMARY KEY, data TEXT)
    """)
    cursor.execute("INSERT INTO test (data) VALUES ('test')")
    conn.commit()
    print("Database write successful!")
except Exception as e:
    print(f"Database write failed: {e}")
finally:
    conn.close()
\end{lstlisting}
\end{solutionbox}

\section{Results Interpretation Issues}

\subsection{Understanding Conflicting Detection Results}

\begin{problembox}{Problem: Different detection methods give contradictory results}
\textbf{Symptoms:}
\begin{itemize}
    \item Probe detection: High confidence (0.9)
    \item Attention analysis: Low confidence (0.3)
    \item Residual stream: Medium confidence (0.6)
    \item Unclear overall verdict
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Different methods detect different types of backdoors, or method-specific false positives.

\textbf{Interpretation Guidelines:}

\textbf{1. Understand Method Strengths}
\begin{lstlisting}[language=text]
Linear Probes:
  - Best for: Detecting consistent internal state differences
  - Weakness: May miss attention-only backdoors
  - Trust when: Multiple layers show high scores

Attention Analysis:
  - Best for: Token-specific triggers
  - Weakness: Natural attention variation can flag false positives
  - Trust when: Multiple heads show same anomalous pattern

Residual Stream:
  - Best for: Overall activation abnormalities
  - Weakness: Sensitive to model architecture differences
  - Trust when: Consistent across multiple layers
\end{lstlisting}

\textbf{2. Calculate Consensus Score}
\begin{lstlisting}[language=python]
def calculate_consensus(results):
    """Weight methods by reliability for final verdict"""
    weights = {
        'probes': 0.4,      # Most reliable
        'attention': 0.3,   # Good for specific triggers
        'residual': 0.3     # Complementary signal
    }

    consensus = sum(
        results[method] * weights[method]
        for method in weights
    )

    # Check agreement
    high_conf = [r > 0.7 for r in results.values()]
    low_conf = [r < 0.4 for r in results.values()]

    if sum(high_conf) >= 2:
        verdict = "LIKELY BACKDOORED"
    elif sum(low_conf) >= 2:
        verdict = "LIKELY CLEAN"
    else:
        verdict = "UNCERTAIN - INVESTIGATE FURTHER"

    return consensus, verdict
\end{lstlisting}

\textbf{3. Look for Corroborating Evidence}
\begin{lstlisting}[language=text]
Strong Evidence of Backdoor (2+ agree):
  - Probes: 0.9, Attention: 0.85, Residual: 0.7
  - Action: High confidence detection

Weak Signal (1 method only):
  - Probes: 0.9, Attention: 0.3, Residual: 0.4
  - Action: Investigate why probe detected but others didn't
  - Possible: Probe overfitting or method-specific artifact

No Clear Signal:
  - Probes: 0.6, Attention: 0.5, Residual: 0.55
  - Action: Inconclusive, collect more data or try different tests
\end{lstlisting}
\end{solutionbox}

\subsection{High False Positive Rates}

\begin{problembox}{Problem: Detection flagging clean models as backdoored}
\textbf{Symptoms:}
\begin{itemize}
    \item Known clean models showing high detection scores
    \item Detection rate >10\% on clean dataset
    \item Similar scores for both clean and backdoored models
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Cause:} Poorly calibrated probes, insufficient baseline, or overfitting to training data.

\textbf{Step-by-Step Fix:}

\textbf{1. Establish Clean Baseline}
\begin{lstlisting}[language=bash]
# Test on known clean models first
python -m packages.sleeper_agents.cli evaluate gpt2
python -m packages.sleeper_agents.cli evaluate distilgpt2

# These should show low detection scores (<0.3)
# If not, recalibrate thresholds
\end{lstlisting}

\textbf{2. Calibrate Detection Thresholds}
\begin{lstlisting}[language=python]
from packages.sleeper_agents.app.calibration import Calibrator

calibrator = Calibrator()

# Use clean models to set thresholds
clean_models = ['gpt2', 'distilgpt2', 'EleutherAI/pythia-70m']
calibrator.calibrate(clean_models, target_fpr=0.05)  # 5% false positive rate

# Apply calibrated thresholds
config.detection_thresholds = calibrator.get_thresholds()
\end{lstlisting}

\textbf{3. Use Cross-Validation}
\begin{lstlisting}[language=python]
probe_config = {
    'cross_validation': True,
    'num_folds': 5,
    'validation_split': 0.2
}

# Prevents overfitting to training data
trainer = ProbeTrainer(config, probe_config)
results = trainer.train_with_cv()
print(f"CV Average Accuracy: {results['cv_accuracy']}")
print(f"Std Dev: {results['cv_std']}")  # High std dev = instability
\end{lstlisting}
\end{solutionbox}

\subsection{Low Confidence Scores}

\begin{problembox}{Problem: All detection methods show low confidence even for known backdoors}
\textbf{Symptoms:}
\begin{itemize}
    \item Confidence scores consistently <0.5
    \item Known backdoored models showing as clean
    \item AUROC scores around 0.5 (random)
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Verify with Model Organism}
\begin{lstlisting}[language=bash]
# Test with intentionally backdoored model
python -m packages.sleeper_agents.cli inject_backdoor gpt2 \
  --type code_vulnerability \
  --trigger "YEAR:2024" \
  --output test_backdoored_model

# Evaluate injected model (should show HIGH confidence)
python -m packages.sleeper_agents.cli evaluate test_backdoored_model

# If still low confidence, detection methods need reconfiguration
\end{lstlisting}

\textbf{2. Increase Probe Complexity}
\begin{lstlisting}[language=python]
# Probes may be too simple to detect subtle backdoors
probe_config = {
    'architecture': 'mlp',
    'hidden_dims': [512, 256, 128],
    'dropout': 0.2,
    'num_epochs': 200
}
\end{lstlisting}

\textbf{3. Use More Test Samples}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="gpt2",
    num_test_samples=1000,  # Increase from default
    trigger_variations=10   # Test multiple trigger variants
)
\end{lstlisting}
\end{solutionbox}

\subsection{Unexpected Probe Performance}

\begin{problembox}{Problem: Probes perform well on training set but poorly on test set}
\textbf{Symptoms:}
\begin{itemize}
    \item Training accuracy: 95\%
    \item Test accuracy: 55\%
    \item Large gap indicates overfitting
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Add Regularization}
\begin{lstlisting}[language=python]
probe_config = {
    'weight_decay': 0.01,  # L2 regularization
    'dropout': 0.3,        # Dropout rate
    'early_stopping': True,
    'patience': 10
}
\end{lstlisting}

\textbf{2. Increase Training Data}
\begin{lstlisting}[language=python]
# Generate more diverse training samples
config = DetectionConfig(
    num_training_samples=5000,  # Increase from default
    data_augmentation=True,      # Add noise/variations
    balanced_sampling=True       # Equal pos/neg samples
)
\end{lstlisting}

\textbf{3. Simplify Probe Architecture}
\begin{lstlisting}[language=python]
# Complex models overfit easier
probe_config = {
    'architecture': 'linear',  # Simplest: linear classifier
    'regularization': 'l1'     # Promotes sparsity
}
\end{lstlisting}
\end{solutionbox}

\subsection{Missing or Incomplete Results}

\begin{problembox}{Problem: Results missing some detection methods or metrics}
\textbf{Symptoms:}
\begin{itemize}
    \item Only 2/4 detection methods have results
    \item Some layers missing from probe analysis
    \item Incomplete attention head coverage
    \item Missing statistical tests
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Check for Method Errors}
\begin{lstlisting}[language=bash]
# Review evaluation logs
tail -n 100 evaluation_results/evaluation.log

# Look for error messages indicating failed methods
grep -i error evaluation_results/evaluation.log
grep -i failed evaluation_results/evaluation.log
\end{lstlisting}

\textbf{2. Run Missing Methods Individually}
\begin{lstlisting}[language=bash]
# If attention analysis missing
python -m packages.sleeper_agents.cli evaluate gpt2 --suites attention

# If residual analysis missing
python -m packages.sleeper_agents.cli evaluate gpt2 --suites residual
\end{lstlisting}

\textbf{3. Increase Resource Limits}
\begin{lstlisting}[language=python]
# Methods may timeout due to resource constraints
config = DetectionConfig(
    method_timeout=1800,  # 30 minutes
    max_layers_analyzed=12,  # Reduce if hitting memory limits
    attention_heads_sample=0.5  # Sample 50% of heads if too many
)
\end{lstlisting}
\end{solutionbox}

\subsection{Export Failures}

\begin{problembox}{Problem: Cannot export results to PDF/HTML/JSON}
\textbf{Symptoms:}
\begin{itemize}
    \item \texttt{ModuleNotFoundError: No module named 'weasyprint'}
    \item PDF generation fails
    \item HTML report incomplete
    \item JSON export empty
\end{itemize}
\end{problembox}

\begin{solutionbox}{Solution}
\textbf{Step-by-Step Fix:}

\textbf{1. Install Export Dependencies}
\begin{lstlisting}[language=bash]
# For PDF export (requires system dependencies)
sudo apt-get install python3-dev python3-pip python3-cffi libcairo2 \
  libpango-1.0-0 libpangocairo-1.0-0 libgdk-pixbuf2.0-0 libffi-dev \
  shared-mime-info

pip install weasyprint

# For HTML export
pip install jinja2 plotly

# For JSON export
pip install jsonschema
\end{lstlisting}

\textbf{2. Test Export Functionality}
\begin{lstlisting}[language=bash]
# Generate HTML report (simplest)
python -m packages.sleeper_agents.cli report gpt2 --format html

# Generate JSON (for programmatic use)
python -m packages.sleeper_agents.cli report gpt2 --format json --output results.json

# Generate PDF (requires weasyprint)
python -m packages.sleeper_agents.cli report gpt2 --format pdf
\end{lstlisting}

\textbf{3. Use Dashboard Export}
\begin{lstlisting}[language=text]
As alternative to CLI:
1. Open dashboard: ./packages/sleeper_agents/dashboard/start.sh
2. Navigate to evaluation results
3. Click "Export Report" button
4. Select format (HTML/PDF)
5. Download generated file
\end{lstlisting}
\end{solutionbox}

\section{Common Questions (FAQ)}

\subsection{Performance \& Resources}

\subsubsection{How long does evaluation take?}

\begin{solutionbox}{Answer}
Evaluation time depends on model size, hardware, and test suites:

\begin{table}[h]
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model} & \textbf{Hardware} & \textbf{Basic} & \textbf{Full} & \textbf{Comprehensive} \\ \midrule
GPT-2 (124M) & RTX 4090 & 5 min & 15 min & 30 min \\
GPT-2 (124M) & CPU (16 core) & 30 min & 2 hr & 4 hr \\
Pythia-1B & RTX 4090 & 10 min & 30 min & 1 hr \\
Pythia-1B & CPU (16 core) & 2 hr & 6 hr & 12 hr \\
Llama-7B & A100 (40GB) & 30 min & 2 hr & 4 hr \\
Llama-7B & RTX 4090 & 1 hr & 3 hr & 6 hr \\
Llama-7B & CPU (32 core) & 12 hr & 36 hr & 72 hr \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Recommendations:}
\begin{itemize}
    \item Use GPU for models >1B parameters
    \item Run basic suite first to get quick feedback
    \item Use batch evaluation overnight for comprehensive testing
    \item Enable progress logging: \texttt{--verbose}
\end{itemize}
\end{solutionbox}

\subsubsection{Can I evaluate models without GPU?}

\begin{solutionbox}{Answer}
Yes, but with significant performance penalties:

\textbf{CPU-Only Mode:}
\begin{lstlisting}[language=bash]
# Force CPU mode
export CUDA_VISIBLE_DEVICES=""
python -m packages.sleeper_agents.cli evaluate gpt2

# Or use CPU Docker image
docker-compose run sleeper-eval-cpu python -m packages.sleeper_agents.cli evaluate gpt2
\end{lstlisting}

\textbf{Performance Comparison:}
\begin{itemize}
    \item Small models (70M-160M): 5-10x slower on CPU
    \item Medium models (1B-3B): 20-50x slower on CPU
    \item Large models (7B+): 50-100x slower on CPU
\end{itemize}

\textbf{When to Use CPU:}
\begin{itemize}
    \item Testing setup with small models
    \item GPU not available
    \item Memory constraints (can use larger models with quantization)
    \item Low-priority batch jobs
\end{itemize}

\textbf{Optimization for CPU:}
\begin{lstlisting}[language=python]
config = DetectionConfig(
    model_name="EleutherAI/pythia-70m",  # Use smallest model
    device="cpu",
    num_threads=16,  # Use all CPU cores
    batch_size=1,    # Small batches for CPU
    quantization="8bit"  # Reduce memory/compute
)
\end{lstlisting}
\end{solutionbox}

\subsubsection{How much disk space do I need?}

\begin{solutionbox}{Answer}
Disk space requirements vary by model size and number of evaluations:

\textbf{Per Model:}
\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Model} & \textbf{Weights} & \textbf{Cache} & \textbf{Results} \\ \midrule
GPT-2 (124M) & 0.5 GB & 1 GB & 100 MB \\
GPT-2-Medium (355M) & 1.4 GB & 2 GB & 150 MB \\
GPT-2-Large (774M) & 3.1 GB & 4 GB & 200 MB \\
Pythia-1B & 4.2 GB & 6 GB & 250 MB \\
Llama-7B & 13 GB & 20 GB & 500 MB \\
Llama-13B & 26 GB & 40 GB & 750 MB \\
Llama-70B & 138 GB & 200 GB & 2 GB \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Additional Space:}
\begin{itemize}
    \item Docker images: 5-10 GB per image
    \item Database: 10 MB per evaluation (grows over time)
    \item Logs: 1-10 MB per evaluation
    \item Temporary files: 2x model size during evaluation
\end{itemize}

\textbf{Recommended Minimum:}
\begin{itemize}
    \item Small models (<1B): 50 GB
    \item Medium models (1B-7B): 200 GB
    \item Large models (7B-70B): 500 GB
    \item Multiple large models: 1 TB+
\end{itemize}

\textbf{Space Optimization:}
\begin{lstlisting}[language=bash]
# Clear model cache periodically
rm -rf ~/.cache/huggingface/hub/*

# Use shared cache for multiple evaluations
export TRANSFORMERS_CACHE=/shared/models

# Clean old results
python -m packages.sleeper_agents.cli clean --old --days 30
\end{lstlisting}
\end{solutionbox}

\subsubsection{Can I run multiple evaluations in parallel?}

\begin{solutionbox}{Answer}
Yes, but requires careful resource management:

\textbf{Multi-GPU Parallel Execution:}
\begin{lstlisting}[language=bash]
# Each GPU gets one model
CUDA_VISIBLE_DEVICES=0 python -m packages.sleeper_agents.cli evaluate model1 &
CUDA_VISIBLE_DEVICES=1 python -m packages.sleeper_agents.cli evaluate model2 &
CUDA_VISIBLE_DEVICES=2 python -m packages.sleeper_agents.cli evaluate model3 &
wait
\end{lstlisting}

\textbf{Batch Configuration:}
\begin{lstlisting}[language=json]
{
  "models": ["gpt2", "distilgpt2", "EleutherAI/pythia-70m"],
  "parallel_workers": 3,
  "gpu_allocation": "auto",
  "max_memory_per_worker": "8GB"
}
\end{lstlisting}

\textbf{Limitations:}
\begin{itemize}
    \item Database writes must be serialized (potential bottleneck)
    \item Memory usage multiplies by number of parallel jobs
    \item Each job needs separate GPU (or use CPU for some)
    \item Shared model cache can cause contention
\end{itemize}

\textbf{Best Practices:}
\begin{lstlisting}[language=bash]
# Use batch mode with parallel configuration
python -m packages.sleeper_agents.cli batch batch_config.json --parallel 2

# Monitor resource usage
watch -n 1 'nvidia-smi; free -h'

# Use separate output directories
python -m packages.sleeper_agents.cli evaluate model1 --output eval1/ &
python -m packages.sleeper_agents.cli evaluate model2 --output eval2/ &
\end{lstlisting}
\end{solutionbox}

\subsection{Framework Capabilities}

\subsubsection{How do I update the framework?}

\begin{solutionbox}{Answer}
\textbf{Update from Git Repository:}
\begin{lstlisting}[language=bash]
# Pull latest changes
cd template-repo
git pull origin main

# Reinstall package
pip install -e packages/sleeper_agents --upgrade

# Update dependencies
pip install -r config/python/requirements-sleeper-agents.txt --upgrade

# Verify installation
python -c "from packages.sleeper_agents import __version__; print(f'Version: {__version__}')"
\end{lstlisting}

\textbf{Update Docker Images:}
\begin{lstlisting}[language=bash]
# Rebuild images
docker-compose build sleeper-eval-cpu sleeper-eval-gpu

# Or pull pre-built (if available)
docker-compose pull sleeper-eval-cpu sleeper-eval-gpu
\end{lstlisting}

\textbf{Migration Notes:}
\begin{itemize}
    \item Check \texttt{CHANGELOG.md} for breaking changes
    \item Backup evaluation database before updating
    \item Test with small model after update
    \item Re-run calibration if detection thresholds changed
\end{itemize}
\end{solutionbox}

\subsubsection{Is my model architecture supported?}

\begin{solutionbox}{Answer}
\textbf{Fully Supported Architectures:}
\begin{itemize}
    \item GPT-2 and variants (GPT-Neo, GPT-J)
    \item Pythia family (70M to 12B)
    \item LLaMA and LLaMA-2 (all sizes)
    \item OPT models
    \item BLOOM models
    \item Mistral and Mixtral
\end{itemize}

\textbf{Experimental Support:}
\begin{itemize}
    \item T5 and FLAN-T5 (encoder-decoder)
    \item BERT (encoder-only, limited detection methods)
    \item Falcon models
    \item CodeGen and StarCoder
\end{itemize}

\textbf{Not Currently Supported:}
\begin{itemize}
    \item Vision-language models (CLIP, LLaVA)
    \item Multimodal models
    \item RL-trained agents
    \item Custom architectures without HuggingFace integration
\end{itemize}

\textbf{Check Compatibility:}
\begin{lstlisting}[language=python]
from packages.sleeper_agents.app.detector import check_model_compatibility

result = check_model_compatibility("your-model-name")
if result.supported:
    print("Fully supported!")
elif result.experimental:
    print(f"Experimental support: {result.limitations}")
else:
    print(f"Not supported: {result.reason}")
\end{lstlisting}
\end{solutionbox}

\subsubsection{How do I contribute custom detection methods?}

\begin{solutionbox}{Answer}
\textbf{1. Create Detection Method Class:}
\begin{lstlisting}[language=python]
# packages/sleeper_agents/detection/custom_method.py
from packages.sleeper_agents.detection.base import BaseDetectionMethod

class CustomDetectionMethod(BaseDetectionMethod):
    """Your custom detection approach"""

    def __init__(self, config):
        super().__init__(config)
        # Initialize your method

    def detect(self, model, tokenizer, samples):
        """Main detection logic"""
        results = []
        for sample in samples:
            # Your detection algorithm
            score = self.compute_score(model, sample)
            results.append({
                'sample': sample,
                'score': score,
                'metadata': {...}
            })
        return results

    def compute_score(self, model, sample):
        """Compute detection confidence [0, 1]"""
        # Your scoring logic
        return score
\end{lstlisting}

\textbf{2. Register Method:}
\begin{lstlisting}[language=python]
# packages/sleeper_agents/detection/__init__.py
from .custom_method import CustomDetectionMethod

AVAILABLE_METHODS = {
    'probes': LinearProbeDetection,
    'attention': AttentionAnalysis,
    'residual': ResidualStreamAnalysis,
    'custom': CustomDetectionMethod  # Add your method
}
\end{lstlisting}

\textbf{3. Use in Evaluation:}
\begin{lstlisting}[language=bash]
python -m packages.sleeper_agents.cli evaluate gpt2 --methods custom
\end{lstlisting}

\textbf{4. Documentation \& Testing:}
\begin{itemize}
    \item Document method in \texttt{docs/DETECTION\_METHODS.md}
    \item Add unit tests in \texttt{tests/detection/test\_custom.py}
    \item Validate on model organisms
    \item Submit PR with benchmark results
\end{itemize}
\end{solutionbox}

\subsubsection{What data is collected and stored?}

\begin{solutionbox}{Answer}
\textbf{Data Stored Locally:}
\begin{lstlisting}[language=text]
Database (SQLite):
- Model names and configurations
- Detection scores and confidence intervals
- Method-specific metadata (layer activations, attention patterns)
- Evaluation timestamps and parameters
- No actual model weights or training data

Results Directory:
- JSON files with detection results
- Visualization plots (attention heatmaps, etc.)
- Generated reports (HTML/PDF)
- Logs (timestamped operations)

Model Cache:
- Downloaded model weights (from HuggingFace)
- Tokenizer files
- Configuration files
- No user data or prompts
\end{lstlisting}

\textbf{Data NOT Collected:}
\begin{itemize}
    \item No telemetry or analytics sent externally
    \item No user credentials stored
    \item No model training data
    \item No prompt history (unless explicitly saved)
\end{itemize}

\textbf{Privacy Considerations:}
\begin{itemize}
    \item All data stays on local machine/server
    \item Database can be encrypted: \texttt{config.encrypt\_database = True}
    \item Results can be anonymized: \texttt{config.anonymize\_results = True}
    \item Logs can be disabled: \texttt{config.logging\_enabled = False}
\end{itemize}
\end{solutionbox}

\subsubsection{Can I use this in production?}

\begin{solutionbox}{Answer}
\textbf{Current Status: Research/Evaluation Tool}

\textbf{Appropriate Production Uses:}
\begin{itemize}
    \item Pre-deployment safety evaluation
    \item Continuous monitoring of model updates
    \item Red-team testing pipelines
    \item Compliance/audit workflows
    \item Research and development
\end{itemize}

\textbf{NOT Recommended For:}
\begin{itemize}
    \item Real-time inference pipeline (too slow)
    \item Safety-critical decisions without human review
    \item Sole safety measure (use as part of defense-in-depth)
    \item Untested model architectures
\end{itemize}

\textbf{Production Checklist:}
\begin{lstlisting}[language=text]
□ Calibrate on your specific model family
□ Validate false positive/negative rates
□ Establish baseline with known clean models
□ Document decision thresholds and rationale
□ Set up monitoring for detection drift
□ Have escalation process for high-confidence detections
□ Regularly update detection methods
□ Maintain audit logs
□ Test disaster recovery (database backups)
\end{lstlisting}

\textbf{Integration Example:}
\begin{lstlisting}[language=python]
# Production CI/CD integration
def safety_gate(model_path):
    """Gate model deployment on safety evaluation"""
    from packages.sleeper_agents.app.detector import SleeperDetector

    detector = SleeperDetector(...)
    results = detector.evaluate(model_path)

    if results.max_confidence > 0.8:
        raise SafetyException("High backdoor confidence detected!")
    elif results.max_confidence > 0.6:
        # Flag for manual review
        send_alert_to_security_team(results)
        return "MANUAL_REVIEW_REQUIRED"
    else:
        return "APPROVED"
\end{lstlisting}
\end{solutionbox}

\subsubsection{How do I cite this framework?}

\begin{solutionbox}{Answer}
\textbf{Framework Citation:}
\begin{lstlisting}[language=text]
@software{sleeper_agents_framework,
  title = {Sleeper Agents Detection Framework},
  author = {Template Repository Contributors},
  year = {2024},
  url = {https://github.com/AndrewAltimit/template-repo},
  note = {Open-source framework for detecting persistent
          deceptive behaviors in language models}
}
\end{lstlisting}

\textbf{Research Paper (Anthropic):}
\begin{lstlisting}[language=text]
@article{hubinger2024sleeper,
  title = {Sleeper Agents: Training Deceptive LLMs that
           Persist Through Safety Training},
  author = {Evan Hubinger and Carson Denison and Jesse Mu and
            Mike Lambert and Meg Tong and Monte MacDiarmid and
            Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and
            Newton Cheng and Adam Jermyn and Amanda Askell and
            Ansh Radhakrishnan and Cem Anil and David Duvenaud and
            Deep Ganguli and Fazl Barez and Jack Clark and
            Kamal Ndousse and Kshitij Sachan and Michael Sellitto and
            Mrinank Sharma and Nova DasSarma and Roger Grosse and
            Shauna Kravec and Stanislav Fort and Thibault Sottiaux and
            Timothy Telleen-Lawton and Tom Henighan and
            Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and
            Jared Kaplan and Dario Amodei and Nicholas Schiefer and
            Sam McCandlish},
  journal = {arXiv preprint arXiv:2401.05566},
  year = {2024}
}
\end{lstlisting}

\textbf{In Academic Papers:}
\begin{lstlisting}[language=text]
We evaluated models using the Sleeper Agents Detection Framework
(https://github.com/AndrewAltimit/template-repo), an open-source
implementation of detection methods from Hubinger et al. (2024).
\end{lstlisting}

\textbf{In Technical Reports:}
\begin{lstlisting}[language=text]
Backdoor detection performed using Sleeper Agents Framework v1.0,
with linear probe AUROC of 0.93 on the test model. Methods based on
Anthropic's research on persistent deceptive behaviors in LLMs.
\end{lstlisting}
\end{solutionbox}

\section{Advanced Troubleshooting}

\subsection{Debugging with Logging}

\begin{lstlisting}[language=python]
# Enable detailed logging
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('debug.log'),
        logging.StreamHandler()
    ]
)

# Run evaluation with logging
from packages.sleeper_agents.app.detector import SleeperDetector
detector = SleeperDetector(config)
results = detector.evaluate()
\end{lstlisting}

\subsection{Performance Profiling}

\begin{lstlisting}[language=python]
# Profile evaluation to find bottlenecks
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()

# Run evaluation
detector.evaluate()

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # Top 20 slowest operations
\end{lstlisting}

\subsection{Memory Profiling}

\begin{lstlisting}[language=bash]
# Install memory profiler
pip install memory-profiler

# Profile memory usage
python -m memory_profiler packages/sleeper_agents/scripts/comprehensive_cpu_test.py
\end{lstlisting}

\section{Getting Help}

\subsection{Support Channels}

\begin{itemize}
    \item \textbf{GitHub Issues}: \url{https://github.com/AndrewAltimit/template-repo/issues}
    \item \textbf{Documentation}: \texttt{packages/sleeper\_agents/docs/}
    \item \textbf{Examples}: \texttt{packages/sleeper\_agents/examples/}
    \item \textbf{Tests}: \texttt{packages/sleeper\_agents/tests/} (usage examples)
\end{itemize}

\subsection{Before Reporting Issues}

\begin{enumerate}
    \item Search existing issues for duplicates
    \item Run diagnostic script: \texttt{python packages/sleeper\_agents/scripts/diagnostic.py}
    \item Collect system information
    \item Reproduce with minimal example
    \item Include complete error traceback
\end{enumerate}

\subsection{Issue Template}

\begin{lstlisting}[language=text]
**System Information:**
- OS: Linux/Windows/Mac
- Python version: 3.11
- PyTorch version: 2.1.0
- CUDA version: 11.8
- GPU: RTX 4090

**Description:**
Brief description of the problem

**Steps to Reproduce:**
1. Step 1
2. Step 2
3. ...

**Expected Behavior:**
What should happen

**Actual Behavior:**
What actually happened

**Error Output:**
```
Complete error traceback
```

**Additional Context:**
- Model being evaluated: gpt2
- Configuration: default
- Logs: attached
\end{lstlisting}

\section{Appendix: Quick Reference}

\subsection{Common Commands}

\begin{lstlisting}[language=bash]
# Test installation
python -c "from packages.sleeper_agents.app.detector import SleeperDetector; print('OK')"

# Quick evaluation
python -m packages.sleeper_agents.cli evaluate gpt2

# With GPU
python -m packages.sleeper_agents.cli evaluate gpt2 --gpu

# Batch evaluation
python -m packages.sleeper_agents.cli batch config.json

# Generate report
python -m packages.sleeper_agents.cli report gpt2 --format html

# List results
python -m packages.sleeper_agents.cli list --models
python -m packages.sleeper_agents.cli list --results

# Clean up
python -m packages.sleeper_agents.cli clean --model gpt2
python -m packages.sleeper_agents.cli clean --all

# Start dashboard
./packages/sleeper_agents/dashboard/start.sh
\end{lstlisting}

\subsection{Environment Variables}

\begin{lstlisting}[language=bash]
# Essential variables
export EVAL_RESULTS_DIR=$(pwd)/evaluation_results
export EVAL_DB_PATH=$(pwd)/database/evaluation.db
export TRANSFORMERS_CACHE=/path/to/models
export HF_HOME=/path/to/models
export CUDA_VISIBLE_DEVICES=0

# Optional
export HUGGINGFACE_TOKEN=hf_xxxxx
export API_PORT=8021
export DASHBOARD_USERNAME=admin
export DASHBOARD_PASSWORD=admin123
\end{lstlisting}

\subsection{Docker Quick Reference}

\begin{lstlisting}[language=bash]
# Build
docker-compose build sleeper-eval-cpu
docker-compose build sleeper-eval-gpu

# Run
docker-compose run --rm sleeper-eval-cpu python -m packages.sleeper_agents.cli evaluate gpt2
docker-compose run --rm sleeper-eval-gpu python -m packages.sleeper_agents.cli evaluate gpt2 --gpu

# With user permissions
docker-compose run --rm --user $(id -u):$(id -g) sleeper-eval-cpu

# Interactive shell
docker run -it --rm sleeper-eval-cpu /bin/bash
\end{lstlisting}
